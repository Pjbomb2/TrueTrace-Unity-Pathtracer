#include "../../GlobalDefines.cginc"
#include "../DenoisersCommon.cginc"
#include "UnityCG.cginc"
#pragma warning( disable : 3556)

float ResRatio;

inline float4x4 inverse(float4x4 m) {
    float n11 = m[0][0], n12 = m[1][0], n13 = m[2][0], n14 = m[3][0];
    float n21 = m[0][1], n22 = m[1][1], n23 = m[2][1], n24 = m[3][1];
    float n31 = m[0][2], n32 = m[1][2], n33 = m[2][2], n34 = m[3][2];
    float n41 = m[0][3], n42 = m[1][3], n43 = m[2][3], n44 = m[3][3];

    float t11 = n23 * n34 * n42 - n24 * n33 * n42 + n24 * n32 * n43 - n22 * n34 * n43 - n23 * n32 * n44 + n22 * n33 * n44;
    float t12 = n14 * n33 * n42 - n13 * n34 * n42 - n14 * n32 * n43 + n12 * n34 * n43 + n13 * n32 * n44 - n12 * n33 * n44;
    float t13 = n13 * n24 * n42 - n14 * n23 * n42 + n14 * n22 * n43 - n12 * n24 * n43 - n13 * n22 * n44 + n12 * n23 * n44;
    float t14 = n14 * n23 * n32 - n13 * n24 * n32 - n14 * n22 * n33 + n12 * n24 * n33 + n13 * n22 * n34 - n12 * n23 * n34;

    float det = n11 * t11 + n21 * t12 + n31 * t13 + n41 * t14;
    float idet = 1.0f / det;

    float4x4 ret;

    ret[0][0] = t11 * idet;
    ret[0][1] = (n24 * n33 * n41 - n23 * n34 * n41 - n24 * n31 * n43 + n21 * n34 * n43 + n23 * n31 * n44 - n21 * n33 * n44) * idet;
    ret[0][2] = (n22 * n34 * n41 - n24 * n32 * n41 + n24 * n31 * n42 - n21 * n34 * n42 - n22 * n31 * n44 + n21 * n32 * n44) * idet;
    ret[0][3] = (n23 * n32 * n41 - n22 * n33 * n41 - n23 * n31 * n42 + n21 * n33 * n42 + n22 * n31 * n43 - n21 * n32 * n43) * idet;

    ret[1][0] = t12 * idet;
    ret[1][1] = (n13 * n34 * n41 - n14 * n33 * n41 + n14 * n31 * n43 - n11 * n34 * n43 - n13 * n31 * n44 + n11 * n33 * n44) * idet;
    ret[1][2] = (n14 * n32 * n41 - n12 * n34 * n41 - n14 * n31 * n42 + n11 * n34 * n42 + n12 * n31 * n44 - n11 * n32 * n44) * idet;
    ret[1][3] = (n12 * n33 * n41 - n13 * n32 * n41 + n13 * n31 * n42 - n11 * n33 * n42 - n12 * n31 * n43 + n11 * n32 * n43) * idet;

    ret[2][0] = t13 * idet;
    ret[2][1] = (n14 * n23 * n41 - n13 * n24 * n41 - n14 * n21 * n43 + n11 * n24 * n43 + n13 * n21 * n44 - n11 * n23 * n44) * idet;
    ret[2][2] = (n12 * n24 * n41 - n14 * n22 * n41 + n14 * n21 * n42 - n11 * n24 * n42 - n12 * n21 * n44 + n11 * n22 * n44) * idet;
    ret[2][3] = (n13 * n22 * n41 - n12 * n23 * n41 - n13 * n21 * n42 + n11 * n23 * n42 + n12 * n21 * n43 - n11 * n22 * n43) * idet;

    ret[3][0] = t14 * idet;
    ret[3][1] = (n13 * n24 * n31 - n14 * n23 * n31 + n14 * n21 * n33 - n11 * n24 * n33 - n13 * n21 * n34 + n11 * n23 * n34) * idet;
    ret[3][2] = (n14 * n22 * n31 - n12 * n24 * n31 - n14 * n21 * n32 + n11 * n24 * n32 + n12 * n21 * n34 - n11 * n22 * n34) * idet;
    ret[3][3] = (n12 * n23 * n31 - n13 * n22 * n31 + n13 * n21 * n32 - n11 * n23 * n32 - n12 * n21 * n33 + n11 * n22 * n33) * idet;

    return ret;
}





#ifdef HDRP
	Texture2DArray<half4> NormalTex;
	Texture2DArray<float2> TEX_PT_MOTION;
    Texture2DArray<float> Depth;
#else
	Texture2D<float2> TEX_PT_MOTION;
	Texture2D<half4> NormalTex;
    Texture2D<float> Depth;
#endif
RWTexture2D<float4> TEX_PT_COLOR_LF_SHWrite;
Texture2D<float4> TEX_PT_COLOR_LF_SH;
RWTexture2D<half4> TEX_PT_COLOR_HFWrite;
Texture2D<half4> TEX_PT_COLOR_HF;
RWTexture2D<uint> TEX_PT_COLOR_SPECWrite;
Texture2D<uint> TEX_PT_COLOR_SPEC;
Texture2D<uint> TEX_ASVGF_GRAD_SMPL_POS_A;
Texture2D<float4> TEX_ASVGF_HIST_COLOR_LF_SH_B;
RWTexture2D<half2> IMG_ASVGF_GRAD_LF_PING;
RWTexture2D<half2> IMG_ASVGF_GRAD_LF_PONG;
RWTexture2D<half2> IMG_ASVGF_GRAD_HF_SPEC_PING;
RWTexture2D<half2> IMG_ASVGF_GRAD_HF_SPEC_PONG;
Texture2D<half> TEX_PT_VIEW_DEPTH_A;//current frame depth
Texture2D<half> TEX_PT_VIEW_DEPTH_B;//previous frame depth
Texture2D<uint> TEX_ASVGF_GRAD_SMPL_POS_B;
RWTexture2D<uint> IMG_ASVGF_GRAD_SMPL_POS_A;

Texture2D<float2> TEX_ASVGF_HIST_COLOR_LF_COCG_B;
Texture2D<half4> TEX_ASVGF_HIST_COLOR_HF;
Texture2D<float2> TEX_ASVGF_FILTERED_SPEC_B;
Texture2D<half4> TEX_ASVGF_HIST_MOMENTS_HF_B;
RWTexture2D<float2> TEX_PT_COLOR_LF_COCGWrite;
Texture2D<float2> TEX_PT_COLOR_LF_COCG;
RWTexture2D<half4> IMG_ASVGF_HIST_MOMENTS_HF_A;
RWTexture2D<float4> IMG_ASVGF_HIST_COLOR_LF_SH_A;
RWTexture2D<float2> IMG_ASVGF_HIST_COLOR_LF_COCG_A;
RWTexture2D<half4> IMG_ASVGF_ATROUS_PING_HF;
RWTexture2D<float2> IMG_ASVGF_ATROUS_PING_SPEC2;
RWTexture2D<half2> IMG_ASVGF_ATROUS_PING_MOMENTS;
RWTexture2D<float4> IMG_ASVGF_ATROUS_PING_LF_SH;
RWTexture2D<float2> IMG_ASVGF_ATROUS_PING_LF_COCG;

RWTexture2D<half2> MetallicAWrite;
Texture2D<half2> MetallicA;
Texture2D<half2> MetallicB;

RWTexture2D<uint> ReflRefracAWrite;
Texture2D<uint> ReflRefracA;
Texture2D<uint> ReflRefracB;

Texture2D<half4> TEX_ASVGF_HIST_MOMENTS_HF_A;


RWTexture2D<uint4> TEX_PT_NORMALS_AWrite;
Texture2D<uint4> TEX_PT_NORMALS_A;
Texture2D<uint4> TEX_PT_NORMALS_B;

Texture2D<float4> TEX_ASVGF_HIST_COLOR_LF_SH_A;

Texture2D<float2> TEX_ASVGF_HIST_COLOR_LF_COCG_A;

Texture2D<half4> TEX_ASVGF_ATROUS_PING_HF;
SamplerState my_point_clamp_sampler;

StructuredBuffer<float> ExposureBuffer;
bool UseExposure;
float IndirectBoost;

static const int GRAD_DWN = 3;
#define STRATUM_OFFSET_SHIFT 3
#define STRATUM_OFFSET_MASK ((1 << STRATUM_OFFSET_SHIFT) - 1)

static const float gaussian_kernel[2][2] = {
	{ 1.0 / 4.0, 1.0 / 8.0  },
	{ 1.0 / 8.0, 1.0 / 16.0 }
};

static const float wavelet_factor = 0.5;
static const float wavelet_kernel[2][2] = {
	{ 1.0, wavelet_factor  },
	{ wavelet_factor, wavelet_factor * wavelet_factor }
};


#pragma kernel CopyData






struct SH {
	float4 shY;
	float2 CoCg;
};

inline SH init_SH()
{
	SH result;
	result.shY = 0;
	result.CoCg = 0;
	return result;
}

inline void accumulate_SH(inout SH accum, SH b, float scale)
{
	accum.shY += b.shY * scale;
	accum.CoCg += b.CoCg * scale;
}

inline SH mix_SH(SH a, SH b, float s)
{
	SH result;
	result.shY = lerp(a.shY, b.shY, s);
	result.CoCg = lerp(a.CoCg, b.CoCg, s);
	return result;
}

inline SH load_SH(Texture2D<float4> img_shY, Texture2D<float2> img_CoCg, int2 p)
{
	SH result;
	result.shY = img_shY[p];
	result.CoCg = img_CoCg[p];
	return result;
}

// Use a macro to work around the glslangValidator errors about function argument type mismatch
#define STORE_SH(img_shY, img_CoCg, p, sh) {img_shY[p] = sh.shY; img_CoCg[p] = sh.CoCg; }

inline void store_SH(RWTexture2D<float4> img_shY, RWTexture2D<float2> img_CoCg, int2 p, SH sh)
{
	img_shY[p] = sh.shY;
	img_CoCg[p] = sh.CoCg;
}

Texture2D<int> Normal;

float FarPlane;
RWTexture2D<float4> RNGTexA;
RWTexture2D<float4> RNGTexBWrite;
Texture2D<float4> RNGTexB;

int CurFrame;

SamplerState sampler_trilinear_clamp;


RWTexture2D<float4> ScreenSpaceInfoWrite;

Texture2D<uint4> WorldPosData;

SH irradiance_to_SH(float3 color, float3 dir)
{
	SH result;
	color = log(color + 1);
	float   Co = color.r - color.b;
	float   t = color.b + Co * 0.5;
	float   Cg = color.g - t;
	float   Y = max(t + Cg * 0.5, 0.0);

	result.CoCg = float2(Co, Cg);

	float   L00 = 0.282095;
	float   L1_1 = 0.488603 * dir.y;
	float   L10 = 0.488603 * dir.z;
	float   L11 = 0.488603 * dir.x;

	result.shY = float4 (L11, L1_1, L10, L00) * Y;

	return result;
}

RWTexture2D<uint2> AlbedoColorA;
Texture2D<uint2> AlbedoColorB;



float4x4 viewprojection;
float4x4 prevviewprojection;
int PartialRenderingFactor;
[numthreads(16, 16, 1)]
void CopyData(uint3 id : SV_DispatchThreadID)
{
	if(any(id.xy >= int2(screen_width, screen_height))) return;
	int pixel_index = id.y * screen_width + id.x;
	ColData Pixel = GlobalColorsRead[pixel_index];
	const uint4 ScreenInfo = asuint(ScreenSpaceInfo[id.xy]);
	float3 TexBaseColor = Pixel.Data.xyz;
	uint2 AlbCol = Pixel.Flags;
	if(GetBounceData(Pixel.MetRoughIsSpec) != 1) AlbCol = packRGBE(max(unpackRGBE(AlbCol), 0.01f));
	Pixel.Flags = packRGBE(max(unpackRGBE(Pixel.Flags), 0.01f));
	float2 MetRough = FromColorSpecPacked(Pixel.MetRoughIsSpec);
	if(GetBounceData(Pixel.MetRoughIsSpec) == 1) {
		Pixel.Direct = float3(0,0,0);
		Pixel.Indirect = float3(0,0,0);
		Pixel.PrimaryNEERay = 0;
	}

	#ifdef HDRP
		float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, int3(id.xy,0) / float3(screen_width, screen_height, 1), 0).xy;
	#else
		float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, id.xy / float2(screen_width, screen_height), 0).xy;
	#endif
	
	float2 pos_prev = ((((float2(id.xy)+0.5f) * float2(rcp(screen_width), rcp(screen_height)) + motion) * float2(screen_width, screen_height)));
	bool InvalidReprojection = (any(pos_prev < 0 || pos_prev >= float2(screen_width, screen_height))) || !(((ReflRefracB[pos_prev] << 4) >> 4) == (((ScreenInfo.w << 4) >> 4)));
	bool ReflectedRefracted = max(FromColorSpecPacked(Pixel.MetRoughIsSpec).z - 2,0);
	Pixel.Data.xyz = max(Pixel.Data.xyz, 0.01f) * rcp(unpackRGBE(Pixel.Flags));
	float3 NEEValue = pow(unpackRGBE(Pixel.PrimaryNEERay),2.2f) * (TexBaseColor > 0.005f ? rcp(TexBaseColor) : 1);

	float Exposure = 1;
	if(UseExposure) Exposure = ExposureBuffer[0];
	SH TempSH = { 0,0,0,0,0,0 };
	float4 HF = 0;
	uint Spec = 0;
	bool IsDiffuse = !((ScreenInfo.w >> 31) & 0x1) && (FromColorSpecPacked(Pixel.MetRoughIsSpec).z == 2 || MetRough.y >= 0.6f);

	if (IsDiffuse) {
		if(GetBounceData(Pixel.MetRoughIsSpec) != 1 && !InvalidReprojection) AlbCol.y = AlbedoColorB[pos_prev].y;
		TempSH = irradiance_to_SH(Exposure * PartialRenderingFactor * IndirectBoost * clamp(Pixel.Indirect * Pixel.Data.xyz / 1024.0f,0,1200000.0f), asfloat(WorldPosData[id.xy].xyz));
		HF = float4(PartialRenderingFactor * clamp(Exposure * (Pixel.Direct + NEEValue) * Pixel.Data.xyz,0,1200000.0f),1);
	} else {
		if(GetBounceData(Pixel.MetRoughIsSpec) != 1 && !InvalidReprojection) AlbCol.x = AlbedoColorB[pos_prev].x;
		Spec = packRGBE(PartialRenderingFactor * clamp(Exposure * (Pixel.Direct + IndirectBoost * Pixel.Indirect + NEEValue) * Pixel.Data.xyz,0,1200000.0f));
	}
	STORE_SH(TEX_PT_COLOR_LF_SHWrite, TEX_PT_COLOR_LF_COCGWrite, id.xy, TempSH);
	TEX_PT_COLOR_HFWrite[id.xy] = HF;
	TEX_PT_COLOR_SPECWrite[id.xy] = Spec;
    MetallicAWrite[id.xy] = MetRough;//, ReflectedRefracted, asfloat((asuint(ScreenSpaceInfo[id.xy].w) << 3) >> 3));
    ReflRefracAWrite[id.xy] = (ReflectedRefracted << 31) | ((ScreenInfo.w << 4) >> 4) | ((min(GetBounceData(Pixel.MetRoughIsSpec), 3) << 29));
	AlbedoColorA[id.xy] = AlbCol;
	TEX_PT_NORMALS_AWrite[id.xy] = ScreenInfo.xyxy;
	if(!(!ReflectedRefracted || InvalidReprojection)) {
		if(IsDiffuse) TEX_PT_NORMALS_AWrite[id.xy] = uint4(ScreenInfo.xy, TEX_PT_NORMALS_B[pos_prev].zw);//GeoNorm, Norm
		else TEX_PT_NORMALS_AWrite[id.xy] = uint4(TEX_PT_NORMALS_B[pos_prev].xy, ScreenInfo.xy);//GeoNorm, Norm
	}
}



uint hash_with(uint seed, uint hash) {
	// Wang hash
	seed = (seed ^ 61) ^ hash;
	seed += seed << 3;
	seed ^= seed >> 4;
	seed *= 0x27d4eb2d;
	return seed;
}
uint pcg_hash(uint seed) {
	uint state = seed * 747796405u + 2891336453u;
	uint word = ((state >> ((state >> 28u) + 4u)) ^ state) * 277803737u;
	return (word >> 22u) ^ word;
}

float2 random(uint samdim, int2 id) {
	uint hash = pcg_hash(((id.x + id.y * screen_width) * (uint)112 + samdim));

	const static float one_over_max_unsigned = asfloat(0x2f7fffff);


	float x = hash_with(CurFrame, hash) * one_over_max_unsigned;
	float y = hash_with(CurFrame + 0xdeadbeef, hash) * one_over_max_unsigned;

	return float2(x, y);

}
float CameraDist;


#pragma kernel Reproject

#define GROUP_SIZE_GRAD 8
#define GROUP_SIZE_PIXELS (GROUP_SIZE_GRAD*GRAD_DWN)

groupshared float4 s_reprojected_pixels[GROUP_SIZE_PIXELS][GROUP_SIZE_PIXELS];


inline void reproject_pixel(int2 p, int field_left, int field_right, int2 gl_LocalInvocationID)
{
	int2 local_pos = gl_LocalInvocationID;

	// Initialize the shared memory unconditionally
	s_reprojected_pixels[local_pos.y][local_pos.x] = 0;

	// Compute the previous frame position of this surface
	#ifdef HDRP
		float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, int3(p,0) / float3(screen_width, screen_height, 1), 0).xy;
	#else
		float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, p / float2(screen_width, screen_height), 0).xy;
	#endif
	float2 pos_prev = ((float2(p)+0.5) * float2(rcp(screen_width), rcp(screen_height)) + motion.xy) * float2(screen_width, screen_height);
	int2 pp = int2(floor(pos_prev));

	if (pp.x <= field_left || pp.y <= field_left || pp.x >= field_right || pp.y >= screen_height)
		return;

	// Fetch the previous frame gradient position...
	int2 pos_grad_prev = pp / GRAD_DWN;

	uint prev_grad_sample_pos = TEX_ASVGF_GRAD_SMPL_POS_B[pp / GRAD_DWN].x;
	int2 stratum_prev = int2(
		prev_grad_sample_pos >> (STRATUM_OFFSET_SHIFT * 0),
		prev_grad_sample_pos >> (STRATUM_OFFSET_SHIFT * 1)) & STRATUM_OFFSET_MASK;

	// If this pixel was a gradient on the previous frame, don't use it. Two reasons:
	// 1) Carrying forward the same random number sequence over multiple frames introduces bias.
	// 2) Gradient pixels use light lists from the previous frame. If the same pixel was used
	// as a gradient for more than one frame, we would need to keep the light lists from 2+ frames behind.
	if (all((pos_grad_prev * GRAD_DWN + stratum_prev == pp)))
		return;

	// Load the data for surface matching
	// uint cluster_curr = TEX_PT_CLUSTER_A[p].x;
	// uint cluster_prev = TEX_PT_CLUSTER_B[pp].x;
	float depth_curr = TEX_PT_VIEW_DEPTH_A[p];
	float depth_prev = TEX_PT_VIEW_DEPTH_B[pp];

	float dist_depth = (abs(depth_curr - depth_prev) - CameraDist) / abs(depth_curr);

	// Compare the surfaces
	if (dist_depth < 0.1f)
	{
		float3 prev_hf = TEX_PT_COLOR_HF[pp];
		float3 prev_spec = unpackRGBE(TEX_PT_COLOR_SPEC[pp]);
		float2 prev_lum = float2(luminance(prev_hf), luminance(prev_spec));
		// Store the results into shared memory: previous frame position and luminances
		s_reprojected_pixels[local_pos.y][local_pos.x] = float4(pp, prev_lum);
	}
}

float3 CamDiff;

Texture2D<uint4> PrimaryTriData;

bool UseBackupPointSelection;
StructuredBuffer<uint> MeshIndexes;
[numthreads(GROUP_SIZE_PIXELS, GROUP_SIZE_PIXELS, 1)]
void Reproject(uint3 gl_GlobalInvocationID : SV_DispatchThreadID, uint3 gl_WorkGroupID : SV_GroupID, uint gl_LocalInvocationIndex : SV_GroupIndex, uint3 gl_LocalInvocationID : SV_GroupThreadID)
{
	// First pass: the entire thread group is busy matching pixels with the previous frame

	int2 ipos = gl_GlobalInvocationID;
	int2 pos_grad = ipos / GRAD_DWN;


	reproject_pixel(ipos, 0, screen_width, gl_LocalInvocationID);

	if(ipos.x < screen_width && ipos.y < screen_height) {
		RNGTexA[ipos] = float4(CurFrame, ipos.x + ipos.y * screen_width, 0, 0);
		GlobalRaysMini[ipos.x + ipos.y * screen_width] = CreateCameraRay(ipos / float2(screen_width, screen_height) * 2.0f - 1.0f);
	}
	GroupMemoryBarrierWithGroupSync();

	// Second pass: the first (GROUP_SIZE_GRAD)^2 pixels are looking for the brightest
	// matching pixels in each 3x3 square.

	// Picking the brightest pixel helps prevent bright trails when the light has moved.
	// If we just pick a random pixel in the the penumbra of the sun light for example,
	// there is a high chance that this pixel will not receive any sun light due to random sampling of the sun. 
	// Overall, we'll miss the changing luminance of the moving penumbra, which is very well visible.

	int2 local_pos;
	local_pos.x = int(gl_LocalInvocationIndex) % GROUP_SIZE_GRAD;
	local_pos.y = int(gl_LocalInvocationIndex) / GROUP_SIZE_GRAD;

	if (local_pos.y >= GROUP_SIZE_GRAD)
		return;

	pos_grad = gl_WorkGroupID * GROUP_SIZE_GRAD + local_pos;
	ipos = pos_grad * GRAD_DWN;

	bool found = false;
	int2 found_offset = 0;
	int2 found_pos_prev = 0;
	float2 found_prev_lum = 0;

	[unroll]for (int offy = 0; offy < GRAD_DWN; offy++)
	{
		[unroll]for (int offx = 0; offx < GRAD_DWN; offx++)
		{
			int2 p = local_pos * GRAD_DWN + int2(offx, offy);

			float4 reprojected_pixel = s_reprojected_pixels[p.y][p.x];

			float2 prev_lum = reprojected_pixel.zw;
			// Use total luminance of diffuse and specular as the heuristic
			if (prev_lum.x + prev_lum.y > found_prev_lum.x + found_prev_lum.y)
			{
				found_prev_lum = prev_lum;
				found_offset = int2(offx, offy);
				found_pos_prev = int2(reprojected_pixel.xy);
				found = true;
			}
		}
	}

	if (!found)
	{
		IMG_ASVGF_GRAD_SMPL_POS_A[pos_grad] = 0u;
		return;
	}

	// Final pass: store the gradient information and patch the surface parameters

	ipos += found_offset;

	uint gradient_idx =
		(1 << 31) /* mark sample as busy */
		| (found_offset.x << (STRATUM_OFFSET_SHIFT * 0)) /* encode pos in */
		| (found_offset.y << (STRATUM_OFFSET_SHIFT * 1)); /* current frame */

	IMG_ASVGF_GRAD_SMPL_POS_A[pos_grad] = gradient_idx;
	IMG_ASVGF_GRAD_HF_SPEC_PING[pos_grad] = found_prev_lum;

	RNGTexA[ipos] = float4(RNGTexB[found_pos_prev].xyz, (ReflRefracB[found_pos_prev] >> 31) ? 2 : 1);
	GlobalRaysMini[ipos.x + ipos.y * screen_width] = RayB[found_pos_prev.x + found_pos_prev.y * screen_width];
	uint4 Target = PrimaryTriData[found_pos_prev];
	float3 Pos;
	if(Target.w == 1) Pos = asfloat(Target.xyz);
	else { 
		if(UseBackupPointSelection || Target.x == 9999999) Pos = GlobalRaysMini[ipos.x + ipos.y * screen_width].origin + GlobalRaysMini[ipos.x + ipos.y * screen_width].direction * TEX_PT_VIEW_DEPTH_B[found_pos_prev].x;
		else {
			MyMeshDataCompacted Mesh = _MeshData[Target.x];
			Target.y += Mesh.TriOffset;
			float2 UV;
			UV.x = asfloat(Target.z);//(Target.z & 0xffff) / 65535.0f;
			UV.y = asfloat(Target.w);//(Target.z >> 16) / 65535.0f;
			float4x4 Inverse = inverse(Mesh.W2L);
			Pos = mul(Inverse, float4(AggTris[Target.y].pos0 + UV.x * AggTris[Target.y].posedge1 + UV.y * AggTris[Target.y].posedge2,1)).xyz;
		}
	}

	GlobalRaysMini[ipos.x + ipos.y * screen_width].origin -= CamDiff;
	GlobalRaysMini[ipos.x + ipos.y * screen_width].direction = normalize(Pos - GlobalRaysMini[ipos.x + ipos.y * screen_width].origin);

	MetallicAWrite[ipos] = MetallicB[found_pos_prev];
	ScreenSpaceInfoWrite[ipos] = float4(asfloat(TEX_PT_NORMALS_B[found_pos_prev].xy),0,0);
}


#pragma kernel Gradient_Img

Texture2D<half2> TEX_ASVGF_GRAD_LF_PONG;


inline float get_gradient(float l_curr, float l_prev)
{
	float l_max = max(l_curr, l_prev);

	if (l_max == 0)
		return 0;

	float ret = abs(l_curr - l_prev) / l_max;
	// ret = min(ret, 0.4f);
	ret *= ret; // make small changes less significant

	return ret;
}

inline float2 get_lf_gradient(int2 ipos)
{
	// Find the same surface on the pvreious frame
	#ifdef HDRP
		float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, int3(ipos,0) / float3(screen_width, screen_height, 1), 0).xy;
	#else
		float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, ipos / float2(screen_width, screen_height), 0).xy;
	#endif

	int2 pos_prev = int2(floor(((float2(ipos)+0.5) * float2((rcp(screen_width)), rcp(screen_height)) + motion.xy) * float2(screen_width, screen_height)));

	// Ignore if the surface was outside of the screen
	if (pos_prev.x < 0 || pos_prev.x >= screen_width || pos_prev.y < 0 || pos_prev.y >= screen_height)
		return 0;

	// Get the current path tracer output and the temporally accumulated history.
	// Ignore disocclusion, doesn't seem to be necessary here as there is a huge blur pass
	// done on the LF gradient samples after.
	float lum_curr = TEX_PT_COLOR_LF_SH[ipos].w;
	float lum_prev = TEX_ASVGF_HIST_COLOR_LF_SH_B[pos_prev].w;

	// Return raw colors, do not divide until after the blur pass. We want to detect 
	// brightness changes over large parts of the screen to avoid noise.
	return float2(lum_curr, lum_prev);
}

[numthreads(16, 16, 1)]
void Gradient_Img(uint3 gl_GlobalInvocationID : SV_DispatchThreadID)
{
	int2 ipos = gl_GlobalInvocationID.xy;
	if (any((ipos >= int2(screen_width, screen_height) / GRAD_DWN)))
		return;

	const uint u = TEX_ASVGF_GRAD_SMPL_POS_A[ipos];

	float2 grad_lf = 0;
	float grad_hf = 0;
	float grad_spec = 0;

	// Process reprojected HF and SPEC samples
	if (u != 0u)
	{
		/* position of sample inside of stratum in the current frame */
		const int2 grad_strata_pos = int2(
			u >> (STRATUM_OFFSET_SHIFT * 0),
			u >> (STRATUM_OFFSET_SHIFT * 1)) & STRATUM_OFFSET_MASK;

		/* full position in current frame for gradient sample */
		int2 grad_sample_pos_curr = ipos * GRAD_DWN + grad_strata_pos;

		float2 prev_hf_spec_lum = IMG_ASVGF_GRAD_HF_SPEC_PING[ipos].rg;

		float3 curr_hf = TEX_PT_COLOR_HFWrite[grad_sample_pos_curr];
		float3 curr_spec = unpackRGBE(TEX_PT_COLOR_SPECWrite[grad_sample_pos_curr]);
		TEX_PT_COLOR_HFWrite[grad_sample_pos_curr] = 0;//TEX_PT_COLOR_HFWrite[grad_sample_pos_curr + 1];
		TEX_PT_COLOR_SPECWrite[grad_sample_pos_curr] = 0;//TEX_PT_COLOR_SPECWrite[grad_sample_pos_curr + 1];

		grad_hf = get_gradient(luminance(curr_hf), prev_hf_spec_lum.x);
		grad_spec = get_gradient(luminance(curr_spec), prev_hf_spec_lum.y);
		// grad_hf *= grad_hf > 0.01f;
		// grad_spec *= grad_spec > 0.01f;
		// First person weapon moves a lot, which creates gradients and makes noise.
		// Slow-changing lighting is better than noise on the weapon, so reduce the gradients.
	}

	// Process all LF samples in the 3x3 square, accumulate the luminances

	[unroll]for (int yy = 0; yy < GRAD_DWN; yy++) {
		[unroll]for (int xx = 0; xx < GRAD_DWN; xx++) {
			grad_lf += get_lf_gradient(ipos * GRAD_DWN + int2(xx, yy));
		}
	}

	IMG_ASVGF_GRAD_LF_PING[ipos] = grad_lf;

	IMG_ASVGF_GRAD_HF_SPEC_PING[ipos] = float2(grad_hf, grad_spec);
}





Texture2D<half2> TEX_ASVGF_GRAD_LF_PING;
Texture2D<half2> TEX_ASVGF_GRAD_HF_SPEC_PING;
Texture2D<half2> TEX_ASVGF_GRAD_HF_SPEC_PONG;


#pragma kernel Gradient_Atrous

int iteration;

inline float2 filter_image(Texture2D<half2> img, int2 ipos)
{
	const int2 grad_size = int2(screen_width, screen_height) / GRAD_DWN;

	float2 color_center = img[ipos].xy;

	float sum_w = 1;

	const int step_size = int(1u << iteration);

	float2 sum_color = 0;
	sum_w = 0;
	// ipos += (random(64, ipos) - 0.5f);

	[unroll]for (int yy = -1; yy <= 1; yy++) {
		[unroll]for (int xx = -1; xx <= 1; xx++) {
			int2 p = ipos + int2(xx, yy) * step_size;

			float2  c = img[p].xy;

			if (any((p >= grad_size)))
				c = 0;

			float w = wavelet_kernel[abs(xx)][abs(yy)];// / (float)step_size;

			sum_color += c * w;
			sum_w += w;
		}
	}
	sum_color /= sum_w;


	return sum_color;
}

[numthreads(16, 16, 1)]
void Gradient_Atrous(uint3 id : SV_DispatchThreadID)
{

	int2 ipos = id.xy;
	int2 grad_size = int2(screen_width, screen_height) / GRAD_DWN;
	if (any((ipos >= grad_size)))
		return;

	float2 filtered_lf = 0;
	float2 filtered_hf_spec = 0;
	if(iteration < 3) {
		IMG_ASVGF_GRAD_HF_SPEC_PING[ipos] = filter_image(TEX_ASVGF_GRAD_HF_SPEC_PING, id.xy);
	}
	if(iteration < 6) {
		filtered_lf = filter_image(TEX_ASVGF_GRAD_LF_PING, id.xy);
	} else {
		filtered_lf = filter_image(TEX_ASVGF_GRAD_LF_PING, id.xy);
		filtered_lf.x = get_gradient((exp(filtered_lf.x) - 1) * 1024.0f * 1024.0f, (exp(filtered_lf.y) - 1) * 1024.0f * 1024.0f);
		filtered_lf.y = 0;
	}
	IMG_ASVGF_GRAD_LF_PING[ipos] = filtered_lf;

}


#define GROUP_SIZE 15
// spatially compute variance in a 3x3 (radius = 1) or a 5x5 (radius = 2) window 
#define FILTER_RADIUS 1 
// size of the shared memory copies of color, depth, and normals
#define SHARED_SIZE (GROUP_SIZE + FILTER_RADIUS * 2)



#pragma kernel Temporal

groupshared float4 s_normal_lum[SHARED_SIZE][SHARED_SIZE];
groupshared float s_depth[SHARED_SIZE][SHARED_SIZE];
groupshared float4 s_lf_shy[GROUP_SIZE][GROUP_SIZE];
groupshared float2 s_lf_cocg[GROUP_SIZE][GROUP_SIZE];





inline void preload(int2 gl_WorkGroupID, int gl_LocalInvocationIndex)
{
	int2 groupBase = gl_WorkGroupID * GROUP_SIZE - FILTER_RADIUS;

	// The size of these shared memory buffers is larger than the group size because we 
	// use them for some spatial filtering. So a single load per thread is not enough.
	// Rename the threads so that some of them will load 2 pixels, and most will load 1 pixel, 
	// in the most dense way possible.
	[unroll]for (uint linear_idx = gl_LocalInvocationIndex; linear_idx < SHARED_SIZE * SHARED_SIZE; linear_idx += GROUP_SIZE * GROUP_SIZE)
	{
		// Convert the linear index to 2D index in a SHARED_SIZE x SHARED_SIZE virtual group
		float t = (float(linear_idx) + 0.5) / float(SHARED_SIZE);
		int xx = int(floor(frac(t) * float(SHARED_SIZE)));
		int yy = int(floor(t));

		// Load
		int2 ipos = groupBase + int2(xx, yy);
		float depth = TEX_PT_VIEW_DEPTH_A[ipos];
		float3 normal = i_octahedral_32(TEX_PT_NORMALS_A[ipos].y);
		float3 color_hf = TEX_PT_COLOR_HF[ipos];

		// Store
		s_normal_lum[yy][xx] = float4(normal.xyz, luminance(color_hf.rgb));//packHalf4x16(float4(normal.xyz, luminance(color_hf.rgb)));
		s_depth[yy][xx] = depth;
	}
}


inline void get_shared_data(int2 offset, inout float depth, inout float3 normal, inout float lum_hf, int2 gl_LocalInvocationID)
{
	int2 addr = gl_LocalInvocationID + int2(FILTER_RADIUS, FILTER_RADIUS) + offset;

	float4 normal_lum = s_normal_lum[addr.y][addr.x];
	depth = s_depth[addr.y][addr.x];

	normal = normal_lum.xyz;
	lum_hf = normal_lum.w;
}

Texture2D<float4> TEX_ASVGF_ATROUS_PONG_LF_SH;
Texture2D<float2> TEX_ASVGF_ATROUS_PONG_LF_COCG;

float3 CamDelta;

[numthreads(GROUP_SIZE, GROUP_SIZE, 1)]
void Temporal(uint3 gl_GlobalInvocationID : SV_DispatchThreadID, uint3 gl_WorkGroupID : SV_GroupID, uint gl_LocalInvocationIndex : SV_GroupIndex, uint3 gl_LocalInvocationID : SV_GroupThreadID)
{
	preload(gl_WorkGroupID, gl_LocalInvocationIndex);
	// float CenterDepth = ScreenSpaceInfo[gl_GlobalInvocationID.xy].z;
	GroupMemoryBarrierWithGroupSync();

	int2 ipos = gl_GlobalInvocationID.xy;
	#ifdef HDRP
		float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, int3(ipos,0) / float3(screen_width, screen_height, 1), 0).xy;
	#else
		float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, ipos / float2(screen_width, screen_height), 0).xy;
	#endif


	float2 pos_prev = ((((float2(ipos)+0.5f) * float2(rcp(screen_width), rcp(screen_height)) + motion.xy) * float2(screen_width, screen_height)));// + (random(54, gl_GlobalInvocationID) - 0.5f) * 0.2f;

	// Load the parameters of the target pixel
	bool ReflectedRefracted = ReflRefracA[gl_GlobalInvocationID.xy] >> 31;
	float depth_curr;
	float3 normal_curr;
	float lum_curr_hf;
	get_shared_data(0, depth_curr, normal_curr, lum_curr_hf, gl_LocalInvocationID);
	// Ray ray = CreateCameraRay(ipos / float2(screen_width, screen_height) * 2.0f - 1.0f);
	// float4 curprojectedrefl = mul(viewprojection, float4(ray.origin + ray.direction * CenterDepth, 1));
	// float4 prevprojectedrefl = mul(prevviewprojection, float4(ray.origin + ray.direction * CenterDepth, 1));
	// float2 reflprojection = ((curprojectedrefl.xy / curprojectedrefl.w) - (prevprojectedrefl.xy / prevprojectedrefl.w)) * 0.5f;
	int MatIndex = (ReflRefracA[gl_GlobalInvocationID.xy] << 4) >> 4;

	float motion_length = length((motion.xy) * float2(screen_width, screen_height));

	float shininess = clamp(2.0 / max(pow(MetallicA[ipos].y, 4), 0.001f) - 2.0, 0.0, 32.0);

	float3 geo_normal_curr = i_octahedral_32(TEX_PT_NORMALS_A[ipos].x);
	float3 geo_normal_curr2 = i_octahedral_32(TEX_PT_NORMALS_A[ipos].z);

	// Try to get the history sample for all channels, including HF moments
	bool temporal_sample_valid_diff = false;
	bool temporal_sample_valid_spec = false;
	SH temporal_color_lf = init_SH();
	float3 temporal_color_hf = 0;
	float4 temporal_moments_histlen_hf = 0;
	float4 temporal_color_histlen_spec = 0;
		float temporal_sum_w_diff = 0.0;
		float temporal_sum_w_spec = 0.0;

		float2 pos_ld = floor(pos_prev - 0.5);
		float2 subpix = frac(pos_prev - 0.5 - pos_ld);
	{

		// Bilinear/bilateral filter
		static const int2 off[4] = { { 0, 0 }, { 1, 0 }, { 0, 1 }, { 1, 1 } };
		const float w[4] = {
			(1.0 - subpix.x) * (1.0 - subpix.y),
			(subpix.x) * (1.0 - subpix.y),
			(1.0 - subpix.x) * (subpix.y),
			(subpix.x) * (subpix.y)
		};
		[unroll]for (int i = 0; i < 4; i++) {
			int2 p = int2(pos_ld)+off[i];

			if (p.x < 0 || p.y < 0 || p.x >= screen_width || p.y >= screen_height)
				continue;

			float depth_prev = TEX_PT_VIEW_DEPTH_B[p].x;
			uint4 norms = TEX_PT_NORMALS_B[p];
			bool ReflectedRefractedPrev = ReflRefracB[p] >> 31;
			float3  normal_prev = i_octahedral_32(norms.y);
			float3  geo_normal_prev = i_octahedral_32(norms.x);//needs to be TEX_PT_GEO_NORMAL_B, but since for now I am not worrying about normal maps yet, it can use the same texture
			

			float dist_depth = (abs(depth_curr.x - depth_prev) - CameraDist) / min(depth_curr.x, depth_prev);// * max(depth_curr.y,0.25f);
			float dot_geo_normals = dot(geo_normal_curr, geo_normal_prev);

			if(MatIndex == ((ReflRefracB[p] << 4) >> 4))
			[branch]if(!ReflectedRefracted && !ReflectedRefractedPrev) {
				if (dist_depth < 0.1)
				{
					if(dot_geo_normals > 0.9f) {
						float dot_normals = dot(normal_curr, normal_prev);
						float w_diff = w[i] * max(dot_normals, 0);//pow(max(dot_normals, 0),clamp(6.0f / depth_curr.x,1.0f, 3.0f));// * max(dot_normals, 0);

						SH hist_color_lf = load_SH(TEX_ASVGF_HIST_COLOR_LF_SH_B, TEX_ASVGF_HIST_COLOR_LF_COCG_B, p);
						accumulate_SH(temporal_color_lf, hist_color_lf, w_diff);
		

						temporal_color_hf += TEX_ASVGF_HIST_COLOR_HF[p] * w_diff;
						temporal_moments_histlen_hf += TEX_ASVGF_HIST_MOMENTS_HF_B[p].rgba * w_diff;
						temporal_sum_w_diff += w_diff;
						float w_spec = w[i];// * pow(max(dot_normals, 0), shininess);
						temporal_color_histlen_spec += float4(unpackRGBE(asuint(TEX_ASVGF_FILTERED_SPEC_B[p].x)), TEX_ASVGF_FILTERED_SPEC_B[p].y) * w_spec;
						temporal_sum_w_spec += w_spec;
					} else if(dot(geo_normal_curr2, i_octahedral_32(norms.z)) > 0.9f) {
						float w_spec = w[i];// * pow(max(dot_normals, 0), shininess);
						temporal_color_histlen_spec += float4(unpackRGBE(asuint(TEX_ASVGF_FILTERED_SPEC_B[p].x)), TEX_ASVGF_FILTERED_SPEC_B[p].y) * w_spec;
						temporal_sum_w_spec += w_spec;
					}
				}
			} else {
				if (dist_depth < 0.1)
				{
					float w_diff = pow(w[i],clamp(6.0f / depth_curr.x,1.0f, 3.0f));
					float w_spec = w[i];

					SH hist_color_lf = load_SH(TEX_ASVGF_HIST_COLOR_LF_SH_B, TEX_ASVGF_HIST_COLOR_LF_COCG_B, p);
					accumulate_SH(temporal_color_lf, hist_color_lf, w_diff);

					temporal_color_hf += TEX_ASVGF_HIST_COLOR_HF[p] * w_diff;
					temporal_color_histlen_spec += float4(unpackRGBE(asuint(TEX_ASVGF_FILTERED_SPEC_B[p].x)), TEX_ASVGF_FILTERED_SPEC_B[p].y) * w_spec;
					temporal_moments_histlen_hf += TEX_ASVGF_HIST_MOMENTS_HF_B[p].rgba * w_diff;
					temporal_sum_w_diff += w_diff;
					temporal_sum_w_spec += w_spec;
				}	
			}
		}

		// We found some relevant surfaces - good
		if (temporal_sum_w_diff > 1e-6)
		{
			float inv_w_diff = 1.0 / temporal_sum_w_diff;
			temporal_color_lf.shY *= inv_w_diff;
			temporal_color_lf.CoCg *= inv_w_diff;
			temporal_color_hf *= inv_w_diff;
			temporal_moments_histlen_hf *= inv_w_diff;
			temporal_sample_valid_diff = true;
		}

		if (temporal_sum_w_spec > 1e-6)
		{
			float inv_w_spec = 1.0 / temporal_sum_w_spec;
			temporal_color_histlen_spec *= inv_w_spec;
			temporal_sample_valid_spec = true;
		}

	}


	// Compute spatial moments of the HF channel in a 3x3 window
	float2 spatial_moments_hf = float2(lum_curr_hf, lum_curr_hf * lum_curr_hf);

	{
		float spatial_sum_w_hf = 1.0;
		[unroll]for (int yy = -FILTER_RADIUS; yy <= FILTER_RADIUS; yy++) {
			[unroll]for (int xx = -FILTER_RADIUS; xx <= FILTER_RADIUS; xx++) {
				if (xx == 0 && yy == 0)
					continue;

				int2 p = ipos + int2(xx, yy);

				float depth;
				float3 normal;
				float lum_p_hf;
				get_shared_data(int2(xx, yy), depth, normal, lum_p_hf, gl_LocalInvocationID);
				// lum_p_hf += luminance(unpackRGBE(TEX_PT_COLOR_SPEC[p].x));
				float dist_z = abs(depth_curr - depth) / abs(depth_curr);// * FDepth[ipos] * 120.0f;
				if (dist_z < 2.0) {
					float w_hf = clamp(pow(max(0.0, dot(normal, normal_curr)), 128.0),0,1);

					spatial_moments_hf += float2(lum_p_hf * w_hf, lum_p_hf * lum_p_hf * w_hf);
					spatial_sum_w_hf += w_hf;
				}
			}
		}

		float inv_w2_hf = 1.0f / spatial_sum_w_hf;
		spatial_moments_hf *= inv_w2_hf;
	}

	// Load the target pixel colors for all channels
	SH color_curr_lf = load_SH(TEX_PT_COLOR_LF_SH, TEX_PT_COLOR_LF_COCG, ipos);
	float3 color_curr_hf = TEX_PT_COLOR_HF[ipos];
	float3 color_curr_spec = unpackRGBE(TEX_PT_COLOR_SPEC[ipos].x);

	SH out_color_lf;
	float3 out_color_hf;
	float4 out_color_histlen_spec;
	float4 out_moments_histlen_hf;

	// Load the gradients
	float grad_lf = TEX_ASVGF_GRAD_LF_PONG[ipos / GRAD_DWN].r;
	grad_lf = clamp(grad_lf, 0, 1);
	float2 grad_hf_spec = TEX_ASVGF_GRAD_HF_SPEC_PONG[ipos / GRAD_DWN].rg;
	grad_hf_spec = clamp(grad_hf_spec, 0, 1);
	if (temporal_sample_valid_diff)
	{
		// Compute the antilag factors based on the gradients
		float antilag_alpha_lf = clamp(lerp(1.0, 0.2f * grad_lf, 1.0f), 0, 1);//play with the middle 2 values?
		float antilag_alpha_hf = clamp(lerp(1.0, 1.0f * grad_hf_spec.x, 1.0f), 0, 1);//play with the middle 2 values?

		// Adjust the history length, taking the antilag factors into account
		float hist_len_hf = min(temporal_moments_histlen_hf.b * pow(1.0 - antilag_alpha_hf, 10) + 1.0, 256.0);
		float hist_len_lf = min(temporal_moments_histlen_hf.a * pow(1.0 - antilag_alpha_lf, 10) + 1.0, 256.0);

		// Compute the blending weights based on history length, so that the filter
		// converges faster. I.e. the first frame has weight of 1.0, the second frame 1/2, third 1/3 and so on.
		float alpha_color_lf = max(0.01f, 1.0f / (hist_len_lf));
		float alpha_color_hf = max(0.02f, 1.0f / (hist_len_hf));
		float alpha_moments_hf = max(0.01f, 1.0f / (hist_len_hf));

		// Adjust the blending factors, taking the antilag factors into account again
		alpha_color_lf = lerp(alpha_color_lf, 1.0, antilag_alpha_lf);
		alpha_color_hf = lerp(alpha_color_hf, 1.0, antilag_alpha_hf);
		alpha_moments_hf = lerp(alpha_moments_hf, 1.0, antilag_alpha_hf);

		// Blend!
		out_color_lf = mix_SH(temporal_color_lf, color_curr_lf, alpha_color_lf);
		out_color_hf.rgb = lerp(temporal_color_hf.rgb, color_curr_hf.rgb, alpha_color_hf);

		out_moments_histlen_hf.rg = lerp(temporal_moments_histlen_hf.rg, spatial_moments_hf.rg, alpha_moments_hf);
		out_moments_histlen_hf.b = hist_len_hf;
		out_moments_histlen_hf.a = hist_len_lf;
	}
	else
	{
		// No valid history - just use the current color and spatial moments
		out_color_lf = color_curr_lf;
		out_color_hf.rgb = color_curr_hf;
		out_moments_histlen_hf = float4(spatial_moments_hf, 1, 1);
	}

	if (temporal_sample_valid_spec)
	{
		float3 Max = 0;
		[unroll]for(int x = -1; x <= 1; x++) {
			[unroll]for(int y = -1; y <= 1; y++) {
				if(x == 0 && y == 0) continue;
				float3 col = unpackRGBE(TEX_PT_COLOR_SPEC[ipos + int2(x,y)]);
				Max = max(Max, col);
			}
		}
		color_curr_spec.rgb = exp(clamp(log(color_curr_spec.rgb + 1), 0, log(Max + 15))) - 1;

		// Same sequence as above, only for the specular channel
		float antilag = 0.01f * grad_hf_spec.y * clamp(1.0f - motion_length, 0.01f, 1.0f) + motion_length * 0.0005f;
		// float antilag = grad_hf_spec.y + motion_length * 0.001f;
		float antilag_alpha_spec = clamp(lerp(0.0, antilag, 1), 0, 1);
		float hist_len_spec = min(max(temporal_color_histlen_spec.a * pow(1.0 - antilag_alpha_spec, 10),0) + 1.0, 256.0);
		float alpha_color_spec = max(0.01f, 1.0 / max(hist_len_spec,0));
		alpha_color_spec = lerp(alpha_color_spec, 1.0, antilag_alpha_spec);
		out_color_histlen_spec.rgb = lerp(temporal_color_histlen_spec.rgb, color_curr_spec.rgb, alpha_color_spec);
		out_color_histlen_spec.a = hist_len_spec;
	}
	else
	{
		out_color_histlen_spec = float4(color_curr_spec, 1);
	}


	// Store the outputs for furhter processing by the a-trous HF filter
	IMG_ASVGF_HIST_MOMENTS_HF_A[ipos] = out_moments_histlen_hf;
	STORE_SH(IMG_ASVGF_HIST_COLOR_LF_SH_A, IMG_ASVGF_HIST_COLOR_LF_COCG_A, ipos, out_color_lf);
	IMG_ASVGF_ATROUS_PING_HF[ipos] = float4(out_color_hf,1);
	IMG_ASVGF_ATROUS_PING_SPEC2[ipos] = float2(asfloat(packRGBE(out_color_histlen_spec)), out_color_histlen_spec.a);
	IMG_ASVGF_ATROUS_PING_MOMENTS[ipos] = out_moments_histlen_hf.xy;

	GroupMemoryBarrierWithGroupSync();

	// Store the LF channel into shared memory for averaging
	s_lf_shy[gl_LocalInvocationID.y][gl_LocalInvocationID.x] = out_color_lf.shY;
	s_lf_cocg[gl_LocalInvocationID.y][gl_LocalInvocationID.x] = out_color_lf.CoCg;

	GroupMemoryBarrierWithGroupSync();

	// Comptue a 1/3-resolution version of the LF channel for this group.
	// Use a bilateral filter that takes the center pixel of each 3x3 square as the anchor.

	float2 lowres_local_id;
	lowres_local_id.x = gl_LocalInvocationIndex % (GROUP_SIZE / GRAD_DWN);
	lowres_local_id.y = gl_LocalInvocationIndex / (GROUP_SIZE / GRAD_DWN);

	if (lowres_local_id.y >= (GROUP_SIZE / GRAD_DWN))
		return;

	// Load the anchor pixel info
	float2 center_shared_pos = lowres_local_id * GRAD_DWN + 1;
	float3 center_normal = s_normal_lum[center_shared_pos.y + FILTER_RADIUS][center_shared_pos.x + FILTER_RADIUS].xyz;
	float center_depth = s_depth[center_shared_pos.y + FILTER_RADIUS][center_shared_pos.x + FILTER_RADIUS];

	SH center_lf;
	center_lf.shY = s_lf_shy[center_shared_pos.y][center_shared_pos.x];
	center_lf.CoCg = s_lf_cocg[center_shared_pos.y][center_shared_pos.x];

	float sum_w = 1;
	SH sum_lf = center_lf;

	// Average the anchor pixel color with the relevant neighborhood
	[unroll]for (int yy = -1; yy <= 1; yy++) {
		[unroll]for (int xx = -1; xx <= 1; xx++) {
			if (yy == 0 && xx == 0)
				continue;

			// float3 p_normal = s_normal_lum[center_shared_pos.y + FILTER_RADIUS + yy][center_shared_pos.x + FILTER_RADIUS + xx].xyz;
			float p_depth = s_depth[center_shared_pos.y + FILTER_RADIUS + yy][center_shared_pos.x + FILTER_RADIUS + xx];

			float dist_depth = abs(p_depth - center_depth.x);// * center_depth.y;
			if (dist_depth < 2)
			{
				float w = 1;//pow(max(dot(p_normal, center_normal), 0), 8);

				SH p_lf;
				p_lf.shY = s_lf_shy[center_shared_pos.y + yy][center_shared_pos.x + xx];
				p_lf.CoCg = s_lf_cocg[center_shared_pos.y + yy][center_shared_pos.x + xx];

				accumulate_SH(sum_lf, p_lf, w);
				sum_w += w;
			}
		}
	}

	float inv_w = 1.0f / (sum_w);
	sum_lf.shY *= inv_w;
	sum_lf.CoCg *= inv_w;

	// Store the LF result for further processing by the a-trous LF filter
	int2 ipos_lowres = int2(gl_WorkGroupID.xy) * (GROUP_SIZE / GRAD_DWN) + int2(lowres_local_id);
	STORE_SH(IMG_ASVGF_ATROUS_PING_LF_SH, IMG_ASVGF_ATROUS_PING_LF_COCG, ipos_lowres, sum_lf);
}



#pragma kernel Atrous_LF

int MaxIterations;

uniform bool UseASVGF;
Texture2D<half> LFVarianceA;
RWTexture2D<half> LFVarianceB;

inline float3 project_SH_irradiance(SH sh, float3 N)
{
	float d = dot(sh.shY.xyz, N);
	float Y = 2.0 * (1.023326 * d + 0.886226 * sh.shY.w);
	Y = max(Y, 0.0);

	sh.CoCg *= Y * 0.282095 / (sh.shY.w + 1e-6);

	float   T = Y - sh.CoCg.y * 0.5;
	float   G = sh.CoCg.y + T;
	float   B = T - sh.CoCg.x * 0.5;
	float   R = B + sh.CoCg.x;

	return max(exp(float3(R, G, B)) - 1, 0.0);
}
//							1,2,3,4
static const int STEPS[] = {1,2,3,4,2,1};
inline void filter_image(
	Texture2D<float4> img_lf_shY,
	Texture2D<float2> img_lf_CoCg,
	out SH filtered_lf, int2 gl_GlobalInvocationID)
{
	int2 ipos_lowres = gl_GlobalInvocationID;
	int2 ipos_hires = ipos_lowres * GRAD_DWN + 1;

	// Load the color of the target low-res pixel
	SH color_center_lf = load_SH(img_lf_shY, img_lf_CoCg, ipos_lowres);
	int Iter2 = STEPS[iteration - 1];
	const float CenterMetallic = MetallicA[ipos_hires].x;

	float3 geo_normal_center = i_octahedral_32(TEX_PT_NORMALS_A[ipos_hires].x);
	float depth_center = TEX_PT_VIEW_DEPTH_A[ipos_hires];

	float step_size = int(1u << (Iter2));
	// if(ipos_hires.x > screen_width / 2 && iteration > 0) step_size = (1u << (5 - iteration));
	// if(iteration == 4) step_size = int(1u << (1));
	float hist_len_lf = TEX_ASVGF_HIST_MOMENTS_HF_A[ipos_hires].a;
	float sum_w_lf = 1;
	SH sum_color_lf = color_center_lf;

	float TotVariance = LFVarianceA[ipos_lowres];/// (1.0f + (hist_len_lf / 8.0f));
	float Weight1 = max((exp(img_lf_shY[ipos_lowres].w) - 1) * 1024.0f* 1024.0f,0);

	// Compute the weighted average of color and moments from a sparse 3x3 pattern around the target pixel
	const int r = 1;
	for (int yy = -r; yy <= r; yy++) {
		for (int xx = -r; xx <= r; xx++) {
			if (xx == 0 && yy == 0)
				continue;
			int2 p_lowres = ipos_lowres + int2(xx, yy) * step_size;
			int2 p_hires = p_lowres * GRAD_DWN + 1;


			float w = float(all((p_hires >= int2(0, 0)))
				&& all((p_hires < int2(screen_width, screen_height))));
			if(w == 0) continue;
			// Use geometric normals here so that we can blur over larger areas.
			// The lighting detail will be partially preserved by spherical harmonics.
			float3 geo_normal = i_octahedral_32(TEX_PT_NORMALS_A[p_hires].x);

			float depth = TEX_PT_VIEW_DEPTH_A[p_hires];

			float dist_z = abs(depth_center.x - depth);// * (depth_center.y + 1.0f);
			w *= exp(-dist_z);
			w *= wavelet_kernel[abs(xx)][abs(yy)];
			// w *= (2.0f - (1 + LFVarianceA[p_lowres]));


			float w_lf = w;

			SH c_lf = load_SH(img_lf_shY, img_lf_CoCg, p_lowres);

			float Weight2 = max((exp(img_lf_shY[p_lowres].w) - 1) * 1024.0f* 1024.0f,0);

			float GNdotGN = max(0.0, dot(geo_normal_center, geo_normal));
			w_lf *= pow(GNdotGN, 8);
			float w_l = clamp(exp(-pow(abs(Weight2 - Weight1) / (1.0f + (Weight1)),1) * max(1.0f - LFVarianceA[p_lowres], 0)  * clamp(pow(hist_len_lf / 32.0f,2), 0, 1)),0.01f, 1.0f);
			if ((Weight1 + Weight2) / 2048.0f > 0.01f && Iter2 >= 1 && Iter2 < 5)
				w_lf *= w_l;
			// if(w_lf < 0.1f) continue;
			float2 MetRough = MetallicA[p_hires];
			w_lf *= 1.0f - clamp(abs(MetRough.x - CenterMetallic), 0, 1);
			// The 4th iteration has filter footprint big enough to step over obstacles and produce noticeable light leaking.
			// Prevent that by throwing away samples that are too bright. This also helps make some shadows a bit sharper.
			if (iteration >= 3 && iteration <= 4)
				w_lf *= clamp(1.5 - Weight2 / Weight1 * 0.25, 0, 1);
			TotVariance += LFVarianceA[p_lowres] * w_lf;
			accumulate_SH(sum_color_lf, c_lf, w_lf);
			sum_w_lf += w_lf;
		}
	}
	filtered_lf.shY = sum_color_lf.shY / sum_w_lf;
	filtered_lf.CoCg = sum_color_lf.CoCg / sum_w_lf;
	LFVarianceB[ipos_lowres] = TotVariance / sum_w_lf;
}

inline void deflicker_image(
	Texture2D<float4> img_lf_shY,
	Texture2D<float2> img_lf_CoCg,
	out SH filtered_lf, int2 gl_GlobalInvocationID)
{
	int2 ipos_lowres = gl_GlobalInvocationID;
	SH color_center_lf = load_SH(img_lf_shY, img_lf_CoCg, ipos_lowres);
	int2 ipos_hires = ipos_lowres * GRAD_DWN + 1;

	SH sum_color_lf = init_SH();

	const int r = 1;
	const float num_pixels = pow(r * 2 + 1, 2) - 1;
	float MaxLum = 0;
	float MinLum = 9999.0f;
	for (int yy = -r; yy <= r; yy++) {
		for (int xx = -r; xx <= r; xx++) {
			int2 p_lowres = ipos_lowres + int2(xx, yy);

			if (xx == 0 && yy == 0)
				continue;

			SH c_lf = load_SH(img_lf_shY, img_lf_CoCg, p_lowres);
			MaxLum = max(MaxLum, c_lf.shY.w);
			MinLum = min(MinLum, c_lf.shY.w);
			accumulate_SH(sum_color_lf, c_lf, 1.0);
		}
	}


	float max_lum = sum_color_lf.shY.w * 2 / num_pixels;
	if (color_center_lf.shY.w > max_lum)
	{
		float ratio = max_lum / color_center_lf.shY.w;
		float ratio2 = (exp(max_lum) - 1) * 1024.0f * 1024.0f / ((exp(color_center_lf.shY.w) - 1) * 1024.0f * 1024.0f);
		// if(TEX_ASVGF_HIST_MOMENTS_HF_A[ipos_hires].a > 4) {
			// color_center_lf.shY *= ratio;
			// color_center_lf.CoCg *= ratio;
		// }
		LFVarianceB[gl_GlobalInvocationID.xy] = ratio;
	} else {
		LFVarianceB[gl_GlobalInvocationID.xy] = 0.1f;
	}

	filtered_lf = color_center_lf;
}

Texture2D<float4> TEX_ASVGF_ATROUS_PING_LF_SH;
Texture2D<float2> TEX_ASVGF_ATROUS_PING_LF_COCG;
RWTexture2D<float4> IMG_ASVGF_ATROUS_PONG_LF_SH;
RWTexture2D<float2> IMG_ASVGF_ATROUS_PONG_LF_COCG;


[numthreads(16, 16, 1)]
void Atrous_LF(uint3 gl_GlobalInvocationID : SV_DispatchThreadID, uint3 gl_WorkGroupID : SV_GroupID, uint gl_LocalInvocationIndex : SV_GroupIndex, uint3 gl_LocalInvocationID : SV_GroupThreadID)
{

	int2 ipos = gl_GlobalInvocationID;
	if (any((ipos * GRAD_DWN >= int2(screen_width, screen_height))))
		return;

	SH filtered_lf;

	if(iteration == 0) deflicker_image(TEX_ASVGF_ATROUS_PING_LF_SH, TEX_ASVGF_ATROUS_PING_LF_COCG, filtered_lf, ipos);
	else filter_image(TEX_ASVGF_ATROUS_PING_LF_SH, TEX_ASVGF_ATROUS_PING_LF_COCG, filtered_lf, ipos);

	STORE_SH(IMG_ASVGF_ATROUS_PONG_LF_SH, IMG_ASVGF_ATROUS_PONG_LF_COCG, ipos, filtered_lf);
}


#pragma kernel Atrous



int spec_iteration;

inline float square(float x) { return x * x; }

// Converts a square of roughness to a Phong specular power
inline float RoughnessSquareToSpecPower(in float alpha) {
	return max(0.01, 2.0f / (square(alpha) + 1e-4) - 2.0f);
}

inline void filter_image(
	Texture2D<half4> img_hf,
	Texture2D<uint> img_spec,
	Texture2D<half2> img_moments,
	out float3 filtered_hf,
	out float3 filtered_spec,
	out float2 filtered_moments,
	int2 gl_GlobalInvocationID)
{
	int2 ipos = int2(gl_GlobalInvocationID);

	filtered_hf = img_hf[ipos];
	filtered_spec = unpackRGBE(img_spec[ipos]);
	filtered_moments = img_moments[ipos].xy;

	// if (5 <= spec_iteration)
	// 	return;

	const float depth_center = TEX_PT_VIEW_DEPTH_A[ipos];

	float lum_mean_hf = luminance(filtered_hf);
	float sigma_l_hf = 0;

	float hist_len_hf = TEX_ASVGF_HIST_MOMENTS_HF_A[ipos].b;
	float3 normal_center = normalize(i_octahedral_32(TEX_PT_NORMALS_A[ipos].y));
	float step_size = (1u << (spec_iteration));
	if(spec_iteration > 0) step_size = (1u << (5 - spec_iteration));
	if(spec_iteration != 1) {//should be 4?
		float3 Min = 9999.0f;
		float3 Max = -9999.0f;
		int ValidTaps = 0;
		[unroll]for(int i = -1; i <= 1; i++) {
			[unroll]for(int j = -1; j <= 1; j++) {
				int2 offID = ipos + int2(i, j) * step_size;
				if(i == 0 && j == 0) continue;
				float3 Val = unpackRGBE(img_spec[offID]);
				if(luminance(Val) != 0) {
					ValidTaps++;
				}
					Min = min(Min, Val);
					Max = max(Max, Val);
			}
		}
		if(ValidTaps > 1 && Min.x != 9999.0f) filtered_spec = clamp(filtered_spec, Min, Max);
	}
	[branch]if (hist_len_hf > 0) {
		// Compute luminance variance from the statistical moments: Var(X) = E[X^2] - E[X]^2
		// The `asvgf_temporal` shader computes a combination of temporal and spatial (3x3) moments,
		// and stores these into a texture. This shader will combine moments of the surrounding 
		// pixels using the same weights as for colors, and the combined moments are used on the next iteration.
		lum_mean_hf = filtered_moments.x;
		float lum_variance_hf = max(1e-8, filtered_moments.y - filtered_moments.x * filtered_moments.x);
		sigma_l_hf = min(hist_len_hf, 16) / (2.0 * lum_variance_hf);
	} else {
		// If there is no history, treat all moments as invalid, because 3x3 spatial 
		// is just not enough to get reasonable filtering. Ignore luminance in this case,
		// and perform a depth-normal-guided bilateral blur.
		lum_mean_hf = filtered_moments.x;
		sigma_l_hf = 0.05f;
	}

	// reduce the HF filter sensitivity to normals when the lighting is invalidated
	float normal_weight_scale = clamp(hist_len_hf / 8.0f, 0, 1);

	float normal_weight_hf = 12;
	const float CenterMetallic = MetallicA[ipos].x;
	const float roughness_center = MetallicA[ipos].y;
	normal_weight_hf *= normal_weight_scale;
	float normal_weight_spec = RoughnessSquareToSpecPower(square(roughness_center));
	normal_weight_spec = clamp(normal_weight_spec, 8, 1024);
	normal_weight_spec *= normal_weight_scale;




	float sum_w_hf = 1.0;
	float sum_w_spec = 1.0;

	// Add some jitter to sample positions to hide the a-trous filter aliasing patterns
	// [branch] if(spec_iteration == 4) ipos += int2((random(spec_iteration, gl_GlobalInvocationID.xy) - 0.5f) * step_size);

	float spec_filter_width_scale = clamp(roughness_center * 30 - spec_iteration, 0.1f, 1);


	// Compute the weighted average of color and moments from a sparse 3x3 pattern around the target pixel
	[unroll]for (int xx = -1; xx <= 1; xx++) {
		[unroll]for (int yy = -1; yy <= 1; yy++) {

			[branch]if (xx == 0 && yy == 0)
				continue;
			int2 p = ipos + int2(xx, yy) * step_size;

			float w_hf = all(p >= 0 && p < int2(screen_width, screen_height));
			if(w_hf == 0) continue;

			float3 normal = i_octahedral_32(TEX_PT_NORMALS_A[p].y);
			float depth = TEX_PT_VIEW_DEPTH_A[p];
			float2 MetRough = MetallicA[p];
			float3 c_hf = img_hf[p];
			float3 c_spec = unpackRGBE(img_spec[p]);


			float dist_z = abs(depth_center - depth);
			w_hf *= exp(-dist_z / step_size);
			w_hf *= wavelet_kernel[abs(xx)][abs(yy)];

			float dist_l_hf = abs(lum_mean_hf - luminance(c_hf.rgb));

			w_hf *= exp(-dist_l_hf * dist_l_hf * sigma_l_hf);

			w_hf *= 1.0f - clamp(abs(MetRough.x - CenterMetallic), 0, 1);
			float w_spec = w_hf * max(0, 1.0f - 10.0f * abs(MetRough.y - roughness_center));
			w_spec *= spec_filter_width_scale;

			float NdotN = max(0.0, dot(normal_center, normal));

			if (normal_weight_hf > 0) w_hf *= pow(NdotN, normal_weight_hf);

			if (normal_weight_spec > 0) w_spec *= pow(NdotN, normal_weight_spec);


			float2 c_mom = img_moments[p].xy;
			filtered_hf += c_hf.rgb * w_hf;
			filtered_spec += c_spec.rgb * w_spec;
			filtered_moments += c_mom * w_hf;
			sum_w_hf += w_hf;
			sum_w_spec += w_spec;
		}
	}

	filtered_hf /= sum_w_hf;
	filtered_spec /= sum_w_spec;
	filtered_moments /= sum_w_hf;
}

inline SH interpolate_lf(Texture2D<float4> img_lf_shY, Texture2D<float2> img_lf_CoCg, int2 ipos)
{
	// Target pixel parameters
	float depth_center = TEX_PT_VIEW_DEPTH_A[ipos];
	float3 geo_normal_center = i_octahedral_32(TEX_PT_NORMALS_A[ipos].x);


	float2 pos_lowres = (float2(ipos)+0.5) / GRAD_DWN - 0.5;// + (random(75, ipos) - 0.5f);
	float2 pos_ld = floor(pos_lowres);
	float2 subpix = frac(pos_lowres - pos_ld);

	SH sum_lf = init_SH();
	float sum_w = 0;

	// 4 bilinear taps
	const int2 off[4] = { { 0, 0 }, { 1, 0 }, { 0, 1 }, { 1, 1 } };
	float w[4] = {
		(1.0 - subpix.x) * (1.0 - subpix.y),
		(subpix.x) * (1.0 - subpix.y),
		(1.0 - subpix.x) * (subpix.y),
		(subpix.x) * (subpix.y)
	};
	for (int i = 0; i < 4; i++)
	{
		int2 p_lowres = int2(pos_ld)+off[i];
		int2 p_hires = p_lowres * GRAD_DWN + 1;

		// Low-res pixel parameters
		float p_depth = TEX_PT_VIEW_DEPTH_A[p_hires];
		float3 p_geo_normal = i_octahedral_32(TEX_PT_NORMALS_A[p_hires].x);

		// Start with bilinear weight
		float p_w = w[i];

		// Compute depth and normal similarity between the target pixel and the low-res anchor pixel
		float dist_depth = abs(p_depth - depth_center.x);// * depth_center.y * 1.0f;
		p_w *= exp(-dist_depth);
		p_w *= pow(max(0.0, dot(geo_normal_center, p_geo_normal)), 8);

		if (p_w > 0)
		{
			SH p_lf = load_SH(img_lf_shY, img_lf_CoCg, p_lowres);
			accumulate_SH(sum_lf, p_lf, p_w);
			sum_w += p_w;
		}
	}

	if (sum_w > 0)
	{
		// We found at least one relevant pixel among the 4 bilinear taps - good
		float inv_w = 1.0f / (sum_w);
		sum_lf.shY *= inv_w;
		sum_lf.CoCg *= inv_w;
	}
	else
	{
		// We didn't find anything relevant, so use the full-res temporally filtered LF data instead
		sum_lf = load_SH(TEX_ASVGF_HIST_COLOR_LF_SH_A, TEX_ASVGF_HIST_COLOR_LF_COCG_A, ipos);
	}

	return sum_lf;
}

bool DiffRes;

#ifdef HDRP
	Texture2DArray<float4> DiffuseGBuffer;
	Texture2DArray<float4> SpecularGBuffer;
#else
	Texture2D<float4> DiffuseGBuffer;
	Texture2D<float4> SpecularGBuffer;
#endif

float3 composite_color(float4 surf_base_color,
	float3 projected_lf, float3 high_freq, float3 specular, float3 SpecColor, int2 ipos)
{
	float Exposure = 1;
	if(UseExposure) Exposure = ExposureBuffer[0];
	float3 final_color = ((surf_base_color.w == 1) ? pow(surf_base_color, 1.0f) : ((projected_lf.rgb + high_freq.rgb) * surf_base_color / Exposure + specular * SpecColor / Exposure));

	return final_color;
}

Texture2D<half2> TEX_ASVGF_ATROUS_PING_MOMENTS;
RWTexture2D<uint> IMG_ASVGF_ATROUS_PING_SPEC;
Texture2D<uint> TEX_ASVGF_ATROUS_PING_SPEC;
RWTexture2D<float4> IMG_ASVGF_COLOR;

[numthreads(16, 16, 1)]
void Atrous(uint3 gl_GlobalInvocationID : SV_DispatchThreadID, uint3 gl_WorkGroupID : SV_GroupID, uint gl_LocalInvocationIndex : SV_GroupIndex, uint3 gl_LocalInvocationID : SV_GroupThreadID)
{
	int2 ipos = gl_GlobalInvocationID.xy;
	if (any((ipos >= int2(screen_width, screen_height))))
		return;

	float3 filtered_hf;
	float3 filtered_spec;
	float2 filtered_moments;

	filter_image(TEX_ASVGF_ATROUS_PING_HF, TEX_ASVGF_ATROUS_PING_SPEC, TEX_ASVGF_ATROUS_PING_MOMENTS, filtered_hf, filtered_spec, filtered_moments, ipos);
	

	if(spec_iteration < 4) {
		IMG_ASVGF_ATROUS_PING_HF[ipos] = float4(filtered_hf,1);
		IMG_ASVGF_ATROUS_PING_SPEC[ipos] = packRGBE(filtered_spec);
		IMG_ASVGF_ATROUS_PING_MOMENTS[ipos] = filtered_moments;
	}

	// Perform compositing on the last iteration
	if (spec_iteration == MaxIterations)
	{
		SH filtered_lf = interpolate_lf(TEX_ASVGF_ATROUS_PING_LF_SH, TEX_ASVGF_ATROUS_PING_LF_COCG, ipos);


		float3 normal = i_octahedral_32(TEX_PT_NORMALS_A[ipos].y);
		float4 base_color = float4(unpackRGBE(AlbedoColorB[ipos].x),(ReflRefracA[ipos] << 1) >> 30);// TEX_PT_BASE_COLOR_A.SampleLevel(sampler_trilinear_clamp, ipos / float2(screen_width, screen_height), 0);
		if(DiffRes && base_color.w <= 2) base_color.xyz = 1;
		if(base_color.w == 1) base_color = float4(unpackRGBE(AlbedoColorB[ipos].x),1);
		// Project the spherical harmonics lighting onto the actual surface normal
		float3 projected_lf = project_SH_irradiance(filtered_lf, normal);

		projected_lf *= 1024.0f;
		
		float3 final_color = composite_color(base_color, projected_lf, filtered_hf, filtered_spec, (DiffRes && base_color.w <= 2) ? 1 : unpackRGBE(AlbedoColorB[ipos].y), ipos);
		// SH TempSH = irradiance_to_SH(final_color, i_octahedral_32(TEX_PT_NORMALS_A[ipos].x));
		// final_color = project_SH_irradiance(TempSH, normal);
		IMG_ASVGF_COLOR[ipos] = float4(final_color, 0);
	}
} 


RWTexture2D<half> TEX_PT_VIEW_DEPTH_AWRITE;//current frame depth
float NearPlane;
float3 Forward;
#pragma kernel DistanceCorrectionKernel
float FetchDepth(float2 UV) {
    Ray ray = CreateCameraRay(UV * 2.0f - 1.0f);  
    #ifdef HDRP
        float depth = 1.0f - (Depth[int3(UV * float2(screen_width, screen_height),0)].x);
    #else
        float depth = 1.0f - (Depth.SampleLevel(my_linear_clamp_sampler, UV, 0).x);
    #endif
        depth = NearPlane * FarPlane / (depth * (NearPlane - FarPlane) + FarPlane);
        return length(ray.direction / dot(ray.direction, Forward) * depth);   
}

[numthreads(32, 32, 1)]
void DistanceCorrectionKernel(uint3 id : SV_DispatchThreadID) {
    if(id.x > screen_width || id.y > screen_height) return;
    float2 Uv = (id.xy + 0.5f) / float2(screen_width, screen_height);

    float CurDepth = FetchDepth(Uv);
    // float CurDepthX = FetchDepth(Uv + float2(rcp(screen_width) * 2.0f, 0));
    // float CurDepthY = FetchDepth(Uv + float2(0, rcp(screen_height) * 2.0f));
    // float CurDepthX2 = FetchDepth(Uv - float2(rcp(screen_width) * 2.0f, 0));
    // float CurDepthY2 = FetchDepth(Uv - float2(0, rcp(screen_height) * 2.0f));
    TEX_PT_VIEW_DEPTH_AWRITE[id.xy] = CurDepth;//, abs(CurDepth - CurDepthX) + abs(CurDepth - CurDepthY) + abs(CurDepth - CurDepthY2) + abs(CurDepth - CurDepthX2));
}


#pragma kernel TempCopyKernel

[numthreads(32, 32, 1)]
void TempCopyKernel(uint3 id : SV_DispatchThreadID) {
	IMG_ASVGF_ATROUS_PING_SPEC[id.xy] = asuint(TEX_ASVGF_FILTERED_SPEC_B[id.xy].x);
}