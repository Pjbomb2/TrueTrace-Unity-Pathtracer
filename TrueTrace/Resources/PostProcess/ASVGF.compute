#include "../GlobalDefines.cginc"
#include "UnityCG.cginc"

int screen_width;
int screen_height;


struct ColData {
	float3 throughput;
	float3 Direct;
	float3 Indirect;
	uint Fog;
	int IsSpecular;
	float Metallic;
};
StructuredBuffer<ColData> PerPixelRadiance;

uint packRGBE(float3 v)
{
	float3 va = max(0, v);
	float max_abs = max(va.r, max(va.g, va.b));
	if (max_abs == 0)
		return 0;

	float exponent = floor(log2(max_abs));

	uint result;
	result = uint(clamp(exponent + 20, 0, 31)) << 27;

	float scale = pow(2, -exponent) * 256.0;
	uint3 vu = min(511, round(va * scale));
	result |= vu.r;
	result |= vu.g << 9;
	result |= vu.b << 18;

	return result;
}

float3 unpackRGBE(uint x)
{
	int exponent = int(x >> 27) - 20;
	float scale = pow(2, exponent) / 256.0;

	float3 v;
	v.r = float(x & 0x1ff) * scale;
	v.g = float((x >> 9) & 0x1ff) * scale;
	v.b = float((x >> 18) & 0x1ff) * scale;

	return v;
}

#ifdef HDRP
	Texture2DArray<half4> NormalTex;
	Texture2DArray<float2> TEX_PT_MOTION;
#else
	Texture2D<float2> TEX_PT_MOTION;
	Texture2D<half4> NormalTex;
#endif
Texture2D<float> FDepth;
RWTexture2D<float> FDepthWrite;
RWTexture2D<float4> TEX_PT_COLOR_LF_SHWrite;
Texture2D<float4> TEX_PT_COLOR_LF_SH;
RWTexture2D<uint> TEX_PT_COLOR_HFWrite;
Texture2D<uint> TEX_PT_COLOR_HF;
RWTexture2D<uint> TEX_PT_COLOR_SPECWrite;
Texture2D<uint> TEX_PT_COLOR_SPEC;
Texture2D<float> TEX_ASVGF_GRAD_SMPL_POS_A;
Texture2D<float4> TEX_ASVGF_HIST_COLOR_LF_SH_B;
RWTexture2D<float2> IMG_ASVGF_GRAD_LF_PING;
RWTexture2D<float2> IMG_ASVGF_GRAD_LF_PONG;
RWTexture2D<float2> IMG_ASVGF_GRAD_HF_SPEC_PING;
RWTexture2D<float2> IMG_ASVGF_GRAD_HF_SPEC_PONG;
Texture2D<half> TEX_PT_VIEW_DEPTH_A;//current frame depth
RWTexture2D<half> TEX_PT_VIEW_DEPTH_BWrite;//previous frame depth
Texture2D<half> TEX_PT_VIEW_DEPTH_B;//previous frame depth
Texture2D<float> TEX_ASVGF_GRAD_SMPL_POS_B;
RWTexture2D<float> IMG_ASVGF_GRAD_SMPL_POS_A;

Texture2D<float2> TEX_ASVGF_HIST_COLOR_LF_COCG_B;
Texture2D<uint> TEX_ASVGF_HIST_COLOR_HF;
Texture2D<float2> TEX_ASVGF_FILTERED_SPEC_B;
Texture2D<float4> TEX_ASVGF_HIST_MOMENTS_HF_B;
RWTexture2D<float2> TEX_PT_COLOR_LF_COCGWrite;
Texture2D<float2> TEX_PT_COLOR_LF_COCG;
RWTexture2D<float4> IMG_ASVGF_HIST_MOMENTS_HF_A;
RWTexture2D<float4> IMG_ASVGF_HIST_COLOR_LF_SH_A;
RWTexture2D<float2> IMG_ASVGF_HIST_COLOR_LF_COCG_A;
RWTexture2D<uint> IMG_ASVGF_ATROUS_PING_HF;
RWTexture2D<float2> IMG_ASVGF_ATROUS_PING_SPEC;
RWTexture2D<float2> IMG_ASVGF_ATROUS_PING_MOMENTS;
RWTexture2D<float4> IMG_ASVGF_ATROUS_PING_LF_SH;
RWTexture2D<float2> IMG_ASVGF_ATROUS_PING_LF_COCG;

RWTexture2D<float2> MetallicAWrite;
Texture2D<float2> MetallicA;
Texture2D<float2> MetallicB;

Texture2D<float4> TEX_ASVGF_HIST_MOMENTS_HF_A;


RWTexture2D<int2> TEX_PT_NORMALS_AWrite;
Texture2D<int2> TEX_PT_NORMALS_A;
Texture2D<int2> TEX_PT_NORMALS_B;

Texture2D<float4> TEX_ASVGF_HIST_COLOR_LF_SH_A;

Texture2D<float2> TEX_ASVGF_HIST_COLOR_LF_COCG_A;

RWTexture2D<int> ReflectedRefractedTexWrite;
Texture2D<int> ReflectedRefractedTex;
Texture2D<int> ReflectedRefractedTexPrev;

Texture2D<uint> TEX_ASVGF_ATROUS_PING_HF;
RWTexture2D<float4> DebugTex;
SamplerState my_linear_clamp_sampler;
SamplerState my_point_clamp_sampler;

static const int GRAD_DWN = 3;
#define STRATUM_OFFSET_SHIFT 3
#define STRATUM_OFFSET_MASK ((1 << STRATUM_OFFSET_SHIFT) - 1)

static const float gaussian_kernel[2][2] = {
	{ 1.0 / 1.0, 1.0 / 2.0  },
	{ 1.0 / 2.0, 1.0 / 4.0 }
};

static const float wavelet_factor = 0.75;
static const float wavelet_kernel[2][2] = {
	{ 1.0, wavelet_factor  },
	{ wavelet_factor, wavelet_factor * wavelet_factor }
};

inline float luminance(in float3 color)
{
	return dot(color, float3(0.299, 0.587, 0.114));
}


#pragma kernel CopyData


struct Ray {
	float3 origin;
	float3 direction;
	float3 direction_inv;
};
RWStructuredBuffer<Ray> RayB;
RWStructuredBuffer<Ray> GlobalRays;

inline uint packUnormArb(float3 data2) {
	data2 = (data2 + 1.0f) * 0.5f;
	const uint4 bits = uint4(10, 10, 10, 0);
	const float4 data = float4(data2, 0);
	float4 mull = exp2(float4(bits)) - 1.0;

	uint4 shift = uint4(0, bits.x, bits.x + bits.y, bits.x + bits.y + bits.z);
	uint4 shifted = uint4(data * mull + 0.5) << shift;

	return shifted.x | shifted.y | shifted.z | shifted.w;
}

inline float3 unpackUnormArb(const uint pack) {
	const uint4 bits = uint4(10, 10, 10, 0);
	uint4 maxValue = uint4(exp2(bits) - 1);
	uint4 shift = uint4(0, bits.x, bits.x + bits.y, bits.x + bits.y + bits.z);
	uint4 unshifted = pack >> shift;
	unshifted = unshifted & maxValue;

	return normalize((float4(unshifted).xyz * 1.0 / float4(maxValue).xyz).xyz * 2.0f - 1.0f);
}


struct SH {
	float4 shY;
	float2 CoCg;
};

inline SH init_SH()
{
	SH result;
	result.shY = 0;
	result.CoCg = 0;
	return result;
}

inline void accumulate_SH(inout SH accum, SH b, float scale)
{
	accum.shY += b.shY * scale;
	accum.CoCg += b.CoCg * scale;
}

inline SH mix_SH(SH a, SH b, float s)
{
	SH result;
	result.shY = lerp(a.shY, b.shY, s);
	result.CoCg = lerp(a.CoCg, b.CoCg, s);
	return result;
}

inline SH load_SH(Texture2D<float4> img_shY, Texture2D<float2> img_CoCg, int2 p)
{
	SH result;
	result.shY = img_shY[p];
	result.CoCg = img_CoCg[p];
	return result;
}

// Use a macro to work around the glslangValidator errors about function argument type mismatch
#define STORE_SH(img_shY, img_CoCg, p, sh) {img_shY[p] = sh.shY; img_CoCg[p] = sh.CoCg; }

inline void store_SH(RWTexture2D<float4> img_shY, RWTexture2D<float2> img_CoCg, int2 p, SH sh)
{
	img_shY[p] = sh.shY;
	img_CoCg[p] = sh.CoCg;
}

Texture2D<int> Normal;

float FarPlane;
RWTexture2D<float4> RNGTexA;
RWTexture2D<float4> RNGTexBWrite;
Texture2D<float4> RNGTexB;
Texture2D<half> Depth;

int CurFrame;

Texture2D<half4> TEX_PT_BASE_COLOR_A;
SamplerState sampler_trilinear_clamp;

struct ScreenSpaceData {
	int InitialRayDirection;
	float Roughness;
	int Normal;
	int NormNormal;
	float3 Albedo;
	float Metallic;
	float3 Position;
	float t;
	int MatIndex;
};
StructuredBuffer<ScreenSpaceData> ScreenSpaceInfo;

SH irradiance_to_SH(float3 color, float3 dir)
{
	SH result;
	color = log(color + 1);
	float   Co = color.r - color.b;
	float   t = color.b + Co * 0.5;
	float   Cg = color.g - t;
	float   Y = max(t + Cg * 0.5, 0.0);

	result.CoCg = float2(Co, Cg);

	float   L00 = 0.282095;
	float   L1_1 = 0.488603 * dir.y;
	float   L10 = 0.488603 * dir.z;
	float   L11 = 0.488603 * dir.x;

	result.shY = float4 (L11, L1_1, L10, L00) * Y;

	return result;
}

RWTexture2D<int2> AlbedoColorA;
Texture2D<int2> AlbedoColorB;

float4x4 _CameraInverseProjection;

inline Ray CreateRay(float3 origin, float3 direction) {
	Ray ray;
	ray.origin = origin;
	ray.direction = direction;
	ray.direction_inv = 0;
	return ray;
}



inline Ray CreateCameraRay(float2 uv) {
	// Transform the camera origin to world space
	float3 origin = mul(unity_CameraToWorld, float4(0.0f, 0.0f, 0.0f, 1.0f)).xyz;

	// Invert the perspective projection of the view-space position
	float3 direction = mul(_CameraInverseProjection, float4(uv, 0.0f, 1.0f)).xyz;
	// Transform the direction from camera to world space and normalize
	direction = mul(unity_CameraToWorld, float4(direction, 0.0f)).xyz;
	direction = normalize(direction);

	return CreateRay(origin, direction);
}

float4x4 viewprojection;
float4x4 prevviewprojection;

[numthreads(16, 16, 1)]
void CopyData(uint3 id : SV_DispatchThreadID)
{
	int pixel_index = id.y * screen_width + id.x;
	ColData Pixel = PerPixelRadiance[pixel_index];
	float3 TexBaseColor = TEX_PT_BASE_COLOR_A[id.xy].xyz;
	TexBaseColor = (TexBaseColor > 0.005f ? rcp(TexBaseColor) : 0);
	uint Input = asuint(Pixel.Metallic);
	MetallicAWrite[id.xy] = float2((Input >> 16) / 65535.0f, (Input & 0xFFFF) / 65535.0f);
	if(TEX_PT_BASE_COLOR_A[id.xy].w == 0) {
		Pixel.Direct = float3(0,0,0);
		Pixel.Indirect = float3(0,0,0);
		Pixel.Fog = 0;
	}

	#ifdef HDRP
		float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, int3(id.xy,0) / float3(screen_width, screen_height, 1), 0).xy;
	#else
		float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, id.xy / float2(screen_width, screen_height), 0).xy;
	#endif
	
	float2 pos_prev = ((((float2(id.xy)+0.5f) * float2(rcp(screen_width), rcp(screen_height)) + motion) * float2(screen_width, screen_height)));

	ReflectedRefractedTexWrite[id.xy] = max(Pixel.IsSpecular - 2,0);
	bool ReflectedRefracted = max(Pixel.IsSpecular - 2,0);
	if (Pixel.IsSpecular == 0 || MetallicAWrite[id.xy].y >= 0.4f) {
		AlbedoColorA[id.xy] = int2(packRGBE(ScreenSpaceInfo[pixel_index].Albedo), AlbedoColorB[pos_prev].y);
		STORE_SH(TEX_PT_COLOR_LF_SHWrite, TEX_PT_COLOR_LF_COCGWrite, id.xy, irradiance_to_SH(Pixel.Indirect * ((Pixel.IsSpecular == 2) ? (TEX_PT_BASE_COLOR_A[id.xy].xyz * ((ScreenSpaceInfo[pixel_index].Albedo > 0.005f) ? rcp(ScreenSpaceInfo[pixel_index].Albedo) : 0)) : 1) / 1024.0f, unpackUnormArb(ScreenSpaceInfo[pixel_index].InitialRayDirection)));
		TEX_PT_COLOR_HFWrite[id.xy] = packRGBE((Pixel.Direct + unpackRGBE(Pixel.Fog) * ((Pixel.IsSpecular != 2) ? TexBaseColor : 1)) * ((Pixel.IsSpecular == 2) ? ((ScreenSpaceInfo[pixel_index].Albedo > 0.005f) ? rcp(ScreenSpaceInfo[pixel_index].Albedo) : 1) : 1));
		TEX_PT_COLOR_SPECWrite[id.xy] = 0;
	}
	else {
		AlbedoColorA[id.xy] = int2(AlbedoColorB[pos_prev].x, packRGBE(ScreenSpaceInfo[pixel_index].Albedo));
		SH TempSH = { 0,0,0,0,0,0 };
		STORE_SH(TEX_PT_COLOR_LF_SHWrite, TEX_PT_COLOR_LF_COCGWrite, id.xy, TempSH);
		TEX_PT_COLOR_SPECWrite[id.xy] = packRGBE(max((Pixel.Direct + Pixel.Indirect + unpackRGBE(Pixel.Fog) * TexBaseColor)* TEX_PT_BASE_COLOR_A[id.xy].xyz * ((ScreenSpaceInfo[pixel_index].Albedo > 0.005f) ? rcp(ScreenSpaceInfo[pixel_index].Albedo) : 0),0));
		TEX_PT_COLOR_HFWrite[id.xy] = 0;
	}
	if(!ReflectedRefracted) {
		TEX_PT_NORMALS_AWrite[id.xy] = int2(ScreenSpaceInfo[pixel_index].Normal, ScreenSpaceInfo[pixel_index].NormNormal);//GeoNorm, Norm
	} else {
		TEX_PT_NORMALS_AWrite[id.xy] = TEX_PT_NORMALS_B[pos_prev];
	}
	// DebugTex[id.xy] = float4(unpackUnormArb(TEX_PT_NORMAL_AWrite[id.xy]) * 0.5f + 0.5f, 1);


}

#pragma kernel CopyRadiance




int iter;
float CameraDist;
float3 Forward;
[numthreads(16, 16, 1)]
void CopyRadiance(uint3 id : SV_DispatchThreadID)
{
	if (id.x >= screen_width || id.y >= screen_height) return;
	float2 Uv = id.xy / float2(screen_width, screen_height);
	Ray ray = CreateCameraRay(Uv * 2.0f - 1.0f);
	GlobalRays[id.x + id.y * screen_width] = ray;
	RNGTexA[id.xy] = float4(CurFrame, id.x + id.y * screen_width, 0, 0);
	float CurDepth = Depth.SampleLevel(my_linear_clamp_sampler, Uv, 0);
	float CurDepthX = Depth.SampleLevel(my_linear_clamp_sampler, (Uv + float2(rcp(screen_width), 0)), 0);
	float CurDepthY = Depth.SampleLevel(my_linear_clamp_sampler, (Uv + float2(0, rcp(screen_height))), 0);
		FDepthWrite[id.xy] = abs(CurDepth - CurDepthX) + abs(CurDepth - CurDepthY);
	#ifdef HDRP
		TEX_PT_NORMALS_AWrite[id.xy] = int2(TEX_PT_NORMALS_AWrite[id.xy].x,packUnormArb(normalize(NormalTex[int3(id.xy,0)].xyz * 2.0f - 1.0f)));
	#else
		TEX_PT_NORMALS_AWrite[id.xy] = int2(TEX_PT_NORMALS_AWrite[id.xy].x,packUnormArb(normalize(NormalTex.SampleLevel(my_point_clamp_sampler, Uv, 0).xyz * 2.0f - 1.0f)));
	#endif



}


#pragma kernel Reproject

#define GROUP_SIZE_GRAD 8
#define GROUP_SIZE_PIXELS (GROUP_SIZE_GRAD*GRAD_DWN)

groupshared float4 s_reprojected_pixels[GROUP_SIZE_PIXELS][GROUP_SIZE_PIXELS];


inline void reproject_pixel(int2 p, int field_left, int field_right, int2 gl_LocalInvocationID)
{
	int2 local_pos = gl_LocalInvocationID;

	// Initialize the shared memory unconditionally
	s_reprojected_pixels[local_pos.y][local_pos.x] = 0;

	// Compute the previous frame position of this surface
	#ifdef HDRP
		float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, int3(p,0) / float3(screen_width, screen_height, 1), 0).xy;
	#else
		float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, p / float2(screen_width, screen_height), 0).xy;
	#endif
	float2 pos_prev = ((float2(p)+0.5) * float2(rcp(screen_width), rcp(screen_height)) + motion.xy) * float2(screen_width, screen_height);
	int2 pp = int2(floor(pos_prev));

	if (pp.x < field_left || pp.x >= field_right || pp.y >= screen_height)
		return;

	// Fetch the previous frame gradient position...
	int2 pos_grad_prev = pp / GRAD_DWN;

	uint prev_grad_sample_pos = asuint(TEX_ASVGF_GRAD_SMPL_POS_B[pp / GRAD_DWN].x);
	int2 stratum_prev = int2(
		prev_grad_sample_pos >> (STRATUM_OFFSET_SHIFT * 0),
		prev_grad_sample_pos >> (STRATUM_OFFSET_SHIFT * 1)) & STRATUM_OFFSET_MASK;

	// If this pixel was a gradient on the previous frame, don't use it. Two reasons:
	// 1) Carrying forward the same random number sequence over multiple frames introduces bias.
	// 2) Gradient pixels use light lists from the previous frame. If the same pixel was used
	// as a gradient for more than one frame, we would need to keep the light lists from 2+ frames behind.
	if (all((pos_grad_prev * GRAD_DWN + stratum_prev == pp)))
		return;

	// Load the data for surface matching
	// uint cluster_curr = TEX_PT_CLUSTER_A[p].x;
	// uint cluster_prev = TEX_PT_CLUSTER_B[pp].x;
	float depth_curr = TEX_PT_VIEW_DEPTH_A[p].x;
	float depth_prev = TEX_PT_VIEW_DEPTH_B[pp].x;

	float dist_depth = (abs(depth_curr - depth_prev) - CameraDist) / abs(depth_curr);

	// Compare the surfaces
	if (dist_depth < 0.1f)
	{
		float3 prev_hf = unpackRGBE(TEX_PT_COLOR_HF[pp]);
		float3 prev_spec = unpackRGBE(TEX_PT_COLOR_SPEC[pp]);
		float2 prev_lum = float2(luminance(prev_hf), luminance(prev_spec));
		// Store the results into shared memory: previous frame position and luminances
		s_reprojected_pixels[local_pos.y][local_pos.x] = float4(pp, prev_lum);
	}
}

float3 CamDiff;

[numthreads(GROUP_SIZE_PIXELS, GROUP_SIZE_PIXELS, 1)]
void Reproject(uint3 gl_GlobalInvocationID : SV_DispatchThreadID, uint3 gl_WorkGroupID : SV_GroupID, uint gl_LocalInvocationIndex : SV_GroupIndex, uint3 gl_LocalInvocationID : SV_GroupThreadID)
{
	// First pass: the entire thread group is busy matching pixels with the previous frame

	int2 ipos = gl_GlobalInvocationID;
	int2 pos_grad = ipos / GRAD_DWN;


	reproject_pixel(ipos, 0, screen_width, gl_LocalInvocationID);

	GroupMemoryBarrierWithGroupSync();

	// Second pass: the first (GROUP_SIZE_GRAD)^2 pixels are looking for the brightest
	// matching pixels in each 3x3 square.

	// Picking the brightest pixel helps prevent bright trails when the light has moved.
	// If we just pick a random pixel in the the penumbra of the sun light for example,
	// there is a high chance that this pixel will not receive any sun light due to random sampling of the sun. 
	// Overall, we'll miss the changing luminance of the moving penumbra, which is very well visible.

	int2 local_pos;
	local_pos.x = int(gl_LocalInvocationIndex) % GROUP_SIZE_GRAD;
	local_pos.y = int(gl_LocalInvocationIndex) / GROUP_SIZE_GRAD;

	if (local_pos.y >= GROUP_SIZE_GRAD)
		return;

	pos_grad = gl_WorkGroupID * GROUP_SIZE_GRAD + local_pos;
	ipos = pos_grad * GRAD_DWN;

	bool found = false;
	int2 found_offset = 0;
	int2 found_pos_prev = 0;
	float2 found_prev_lum = 0;

	// DebugTex[pos_grad] = 0;
	for (int offy = 0; offy < GRAD_DWN; offy++)
	{
		for (int offx = 0; offx < GRAD_DWN; offx++)
		{
			int2 p = local_pos * GRAD_DWN + int2(offx, offy);

			float4 reprojected_pixel = s_reprojected_pixels[p.y][p.x];
			// DebugTex[pos_grad] = float4(max(DebugTex[pos_grad].x, reprojected_pixel.z),0,0,0);

			float2 prev_lum = reprojected_pixel.zw;
			// Use total luminance of diffuse and specular as the heuristic
			if (prev_lum.x + prev_lum.y > found_prev_lum.x + found_prev_lum.y)
			{
				found_prev_lum = prev_lum;
				found_offset = int2(offx, offy);
				found_pos_prev = int2(reprojected_pixel.xy);
				found = true;
			}
		}
	}

	if (!found)
	{
		IMG_ASVGF_GRAD_SMPL_POS_A[pos_grad] = asfloat(0u);
		return;
	}

	// Final pass: store the gradient information and patch the surface parameters

	ipos += found_offset;

	uint gradient_idx =
		(1 << 31) /* mark sample as busy */
		| (found_offset.x << (STRATUM_OFFSET_SHIFT * 0)) /* encode pos in */
		| (found_offset.y << (STRATUM_OFFSET_SHIFT * 1)); /* current frame */

	IMG_ASVGF_GRAD_SMPL_POS_A[pos_grad] = asfloat(gradient_idx);
	IMG_ASVGF_GRAD_HF_SPEC_PING[pos_grad] = found_prev_lum;

	RNGTexA[ipos] = float4(RNGTexB[found_pos_prev].xyz, 1);
	GlobalRays[ipos.x + ipos.y * screen_width] = RayB[found_pos_prev.x + found_pos_prev.y * screen_width];
	// float3 Pos = GlobalRays[ipos.x + ipos.y * screen_width].origin + GlobalRays[ipos.x + ipos.y * screen_width].direction * TEX_PT_VIEW_DEPTH_A[ipos].x;
	// GlobalRays[ipos.x + ipos.y * screen_width].origin -= CamDiff;
	// GlobalRays[ipos.x + ipos.y * screen_width].direction = normalize(Pos - GlobalRays[ipos.x + ipos.y * screen_width].origin);
	MetallicAWrite[ipos] = MetallicB[found_pos_prev];

}


#pragma kernel Gradient_Img

Texture2D<float2> TEX_ASVGF_GRAD_LF_PONG;


inline float get_gradient(float l_curr, float l_prev)
{
	float l_max = max(l_curr, l_prev);

	if (l_max == 0)
		return 0;

	float ret = abs(l_curr - l_prev) / l_max;
	ret *= ret; // make small changes less significant

	return ret;
}

inline float2 get_lf_gradient(int2 ipos)
{
	// Find the same surface on the pvreious frame
	#ifdef HDRP
		float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, int3(ipos,0) / float3(screen_width, screen_height, 1), 0).xy;
	#else
		float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, ipos / float2(screen_width, screen_height), 0).xy;
	#endif

	int2 pos_prev = int2(floor(((float2(ipos)+0.5) * float2((rcp(screen_width)), rcp(screen_height)) + motion.xy) * float2(screen_width, screen_height)));

	// Ignore if the surface was outside of the screen
	if (pos_prev.x < 0 || pos_prev.x >= screen_width || pos_prev.y < 0 || pos_prev.y >= screen_height)
		return 0;

	// Get the current path tracer output and the temporally accumulated history.
	// Ignore disocclusion, doesn't seem to be necessary here as there is a huge blur pass
	// done on the LF gradient samples after.
	float lum_curr = TEX_PT_COLOR_LF_SH[ipos].w;//clamp(TEX_PT_COLOR_LF_SH[ipos].w, ClampedMin - 0.1f, ClampedMax + 0.1f);
	float lum_prev = TEX_ASVGF_HIST_COLOR_LF_SH_B[pos_prev].w;

	// Return raw colors, do not divide until after the blur pass. We want to detect 
	// brightness changes over large parts of the screen to avoid noise.
	return float2(lum_curr, lum_prev);
}

[numthreads(16, 16, 1)]
void Gradient_Img(uint3 gl_GlobalInvocationID : SV_DispatchThreadID)
{
	int2 ipos = gl_GlobalInvocationID.xy;
	if (any((ipos >= int2(screen_width, screen_height) / GRAD_DWN)))
		return;

	uint u = asuint(TEX_ASVGF_GRAD_SMPL_POS_A[ipos]);

	float2 grad_lf = 0;
	float grad_hf = 0;
	float grad_spec = 0;

	// Process reprojected HF and SPEC samples
	if (u != 0u)
	{
		/* position of sample inside of stratum in the current frame */
		int2 grad_strata_pos = int2(
			u >> (STRATUM_OFFSET_SHIFT * 0),
			u >> (STRATUM_OFFSET_SHIFT * 1)) & STRATUM_OFFSET_MASK;

		/* full position in current frame for gradient sample */
		int2 grad_sample_pos_curr = ipos * GRAD_DWN + grad_strata_pos;

		float2 prev_hf_spec_lum = IMG_ASVGF_GRAD_HF_SPEC_PING[ipos].rg;

		float3 curr_hf = unpackRGBE(TEX_PT_COLOR_HFWrite[grad_sample_pos_curr]);
		float3 curr_spec = unpackRGBE(TEX_PT_COLOR_SPECWrite[grad_sample_pos_curr]);
		TEX_PT_COLOR_HFWrite[grad_sample_pos_curr] = 0;
		TEX_PT_COLOR_SPECWrite[grad_sample_pos_curr] = 0;
		grad_hf = get_gradient(luminance(curr_hf), prev_hf_spec_lum.x);
		grad_spec = get_gradient(luminance(curr_spec), prev_hf_spec_lum.y);
	// DebugTex[ipos] = float4(grad_hf, 0,0,0);
		// First person weapon moves a lot, which creates gradients and makes noise.
		// Slow-changing lighting is better than noise on the weapon, so reduce the gradients.
	}

	// Process all LF samples in the 3x3 square, accumulate the luminances

	#ifdef HDRP
		float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, int3(ipos,0) / float3(screen_width, screen_height, 1), 0).xy;
	#else
		float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, ipos / float2(screen_width, screen_height), 0).xy;
	#endif

	int2 pos_prev = int2(floor(((float2(ipos)+0.5) * float2((rcp(screen_width / GRAD_DWN)), rcp(screen_height / GRAD_DWN)) + motion.xy) * float2(screen_width, screen_height) / GRAD_DWN));

	// float ClampedMin = TEX_ASVGF_GRAD_LF_PONG[pos_prev].y;
	// float ClampedMax = ClampedMin;
	// for (int yy = -1; yy <= 1; yy++)
	// {
	// 	for (int xx = -1; xx <= 1; xx++)
	// 	{
	// 		if(xx == 0 && yy == 0) continue;
	// 		int2 NewPos = pos_prev + int2(xx,yy);
	// 		ClampedMin = min(ClampedMin, TEX_ASVGF_GRAD_LF_PONG[NewPos].y);
	// 		ClampedMax = max(ClampedMax, TEX_ASVGF_GRAD_LF_PONG[NewPos].y);
	// 	}
	// }
	for (int yy = 0; yy < GRAD_DWN; yy++)
	{
		for (int xx = 0; xx < GRAD_DWN; xx++)
		{
			grad_lf += get_lf_gradient(ipos * GRAD_DWN + int2(xx, yy));
		}
	}

		// DebugTex[ipos] = float4(grad_lf, 0, 1);
	IMG_ASVGF_GRAD_LF_PING[ipos] = grad_lf;

	IMG_ASVGF_GRAD_HF_SPEC_PING[ipos] = float2(grad_hf, grad_spec);
}


uint hash_with(uint seed, uint hash) {
	// Wang hash
	seed = (seed ^ 61) ^ hash;
	seed += seed << 3;
	seed ^= seed >> 4;
	seed *= 0x27d4eb2d;
	return seed;
}
uint pcg_hash(uint seed) {
	uint state = seed * 747796405u + 2891336453u;
	uint word = ((state >> ((state >> 28u) + 4u)) ^ state) * 277803737u;
	return (word >> 22u) ^ word;
}

float2 random(uint samdim, int2 id) {
	uint hash = pcg_hash(((id.x + id.y * screen_width) * (uint)112 + samdim));

	const static float one_over_max_unsigned = asfloat(0x2f7fffff);


	float x = hash_with(CurFrame, hash) * one_over_max_unsigned;
	float y = hash_with(CurFrame + 0xdeadbeef, hash) * one_over_max_unsigned;

	return float2(x, y);

}


Texture2D<float2> TEX_ASVGF_GRAD_LF_PING;
Texture2D<float2> TEX_ASVGF_GRAD_HF_SPEC_PING;
Texture2D<float2> TEX_ASVGF_GRAD_HF_SPEC_PONG;


#pragma kernel Gradient_Atrous

int iteration;

inline float2 filter_image(Texture2D<float2> img, int2 ipos)
{
	int2 grad_size = int2(screen_width, screen_height) / GRAD_DWN;

	float2 color_center = img[ipos].xy;

	float sum_w = 1;

	const int step_size = int(1u << iteration);

	float2 sum_color = 0;
	sum_w = 0;
	ipos += (random(64, ipos) - 0.5f);

	const int r = 1;
	for (int yy = -r; yy <= r; yy++) {
		for (int xx = -r; xx <= r; xx++) {
			int2 p = ipos + int2(xx, yy) * step_size;

			float2  c = img[p].xy;

			if (any((p >= grad_size)))
				c = 0;

			float w = wavelet_kernel[abs(xx)][abs(yy)];// / (float)step_size;

			sum_color += c * w;
			sum_w += w;
		}
	}
	sum_color /= sum_w;


	return sum_color;
}

[numthreads(16, 16, 1)]
void Gradient_Atrous(uint3 id : SV_DispatchThreadID)
{

	int2 ipos = id.xy;
	int2 grad_size = int2(screen_width, screen_height) / GRAD_DWN;
	if (any((ipos >= grad_size)))
		return;

	float2 filtered_lf = 0;
	float2 filtered_hf_spec = 0;
	[branch] switch (iteration) {
	case 0: filtered_lf = filter_image(TEX_ASVGF_GRAD_LF_PING, id.xy); filtered_hf_spec = filter_image(TEX_ASVGF_GRAD_HF_SPEC_PING, id.xy); break;
	case 1: filtered_lf = filter_image(TEX_ASVGF_GRAD_LF_PONG, id.xy); filtered_hf_spec = filter_image(TEX_ASVGF_GRAD_HF_SPEC_PONG, id.xy); break;
	case 2: filtered_lf = filter_image(TEX_ASVGF_GRAD_LF_PING, id.xy); filtered_hf_spec = filter_image(TEX_ASVGF_GRAD_HF_SPEC_PING, id.xy); break;
	case 3: filtered_lf = filter_image(TEX_ASVGF_GRAD_LF_PONG, id.xy); break;
	case 4: filtered_lf = filter_image(TEX_ASVGF_GRAD_LF_PING, id.xy); break;
	case 5: filtered_lf = filter_image(TEX_ASVGF_GRAD_LF_PONG, id.xy); break;
	case 6:
		filtered_lf = filter_image(TEX_ASVGF_GRAD_LF_PING, id.xy);
		filtered_lf.x = get_gradient((exp(filtered_lf.x) - 1) * 1024.0f * 1024.0f, (exp(filtered_lf.y) - 1) * 1024.0f * 1024.0f);
		// filtered_lf.y = 0;
		break;
	}

	[branch] switch (iteration) {
	case 0: IMG_ASVGF_GRAD_LF_PONG[ipos] = filtered_lf; IMG_ASVGF_GRAD_HF_SPEC_PONG[ipos] = filtered_hf_spec; break;
	case 1: IMG_ASVGF_GRAD_LF_PING[ipos] = filtered_lf; IMG_ASVGF_GRAD_HF_SPEC_PING[ipos] = filtered_hf_spec; break;
	case 2: IMG_ASVGF_GRAD_LF_PONG[ipos] = filtered_lf; IMG_ASVGF_GRAD_HF_SPEC_PONG[ipos] = filtered_hf_spec; break;
	case 3: IMG_ASVGF_GRAD_LF_PING[ipos] = filtered_lf; break;
	case 4: IMG_ASVGF_GRAD_LF_PONG[ipos] = filtered_lf; break;
	case 5: IMG_ASVGF_GRAD_LF_PING[ipos] = filtered_lf; break;
	case 6: IMG_ASVGF_GRAD_LF_PONG[ipos] = filtered_lf; break;
	}

}


#define GROUP_SIZE 15
// spatially compute variance in a 3x3 (radius = 1) or a 5x5 (radius = 2) window 
#define FILTER_RADIUS 1 
// size of the shared memory copies of color, depth, and normals
#define SHARED_SIZE (GROUP_SIZE + FILTER_RADIUS * 2)



#pragma kernel Temporal

groupshared float4 s_normal_lum[SHARED_SIZE][SHARED_SIZE];
groupshared float s_depth[SHARED_SIZE][SHARED_SIZE];
groupshared float4 s_lf_shy[GROUP_SIZE][GROUP_SIZE];
groupshared float2 s_lf_cocg[GROUP_SIZE][GROUP_SIZE];
groupshared float s_depth_width[GROUP_SIZE / GRAD_DWN][GROUP_SIZE / GRAD_DWN];





inline void preload(int2 gl_WorkGroupID, int gl_LocalInvocationIndex)
{
	int2 groupBase = gl_WorkGroupID * GROUP_SIZE - FILTER_RADIUS;

	// The size of these shared memory buffers is larger than the group size because we 
	// use them for some spatial filtering. So a single load per thread is not enough.
	// Rename the threads so that some of them will load 2 pixels, and most will load 1 pixel, 
	// in the most dense way possible.
	for (uint linear_idx = gl_LocalInvocationIndex; linear_idx < SHARED_SIZE * SHARED_SIZE; linear_idx += GROUP_SIZE * GROUP_SIZE)
	{
		// Convert the linear index to 2D index in a SHARED_SIZE x SHARED_SIZE virtual group
		float t = (float(linear_idx) + 0.5) / float(SHARED_SIZE);
		int xx = int(floor(frac(t) * float(SHARED_SIZE)));
		int yy = int(floor(t));

		// Load
		int2 ipos = groupBase + int2(xx, yy);
		float depth = TEX_PT_VIEW_DEPTH_A[ipos].x;
		float3 normal = unpackUnormArb(TEX_PT_NORMALS_A[ipos].y);
		float3 color_hf = unpackRGBE(TEX_PT_COLOR_HF[ipos]);

		// Store
		s_normal_lum[yy][xx] = float4(normal.xyz, luminance(color_hf.rgb));//packHalf4x16(float4(normal.xyz, luminance(color_hf.rgb)));
		s_depth[yy][xx] = depth;
	}
}


inline void get_shared_data(int2 offset, out float depth, out float3 normal, out float lum_hf, int2 gl_LocalInvocationID)
{
	int2 addr = gl_LocalInvocationID + int2(FILTER_RADIUS, FILTER_RADIUS) + offset;

	float4 normal_lum = s_normal_lum[addr.y][addr.x];
	depth = s_depth[addr.y][addr.x];

	normal = normal_lum.xyz;
	lum_hf = normal_lum.w;
}

Texture2D<float4> TEX_ASVGF_ATROUS_PONG_LF_SH;
Texture2D<float2> TEX_ASVGF_ATROUS_PONG_LF_COCG;

float3 CamDelta;

[numthreads(GROUP_SIZE, GROUP_SIZE, 1)]
void Temporal(uint3 gl_GlobalInvocationID : SV_DispatchThreadID, uint3 gl_WorkGroupID : SV_GroupID, uint gl_LocalInvocationIndex : SV_GroupIndex, uint3 gl_LocalInvocationID : SV_GroupThreadID)
{
	preload(gl_WorkGroupID, gl_LocalInvocationIndex);
	GroupMemoryBarrierWithGroupSync();

	int2 ipos = gl_GlobalInvocationID.xy;
	#ifdef HDRP
		float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, int3(ipos,0) / float3(screen_width, screen_height, 1), 0).xy;
	#else
		float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, ipos / float2(screen_width, screen_height), 0).xy;
	#endif


	float2 pos_prev = ((((float2(ipos)+0.5f) * float2(rcp(screen_width), rcp(screen_height)) + motion.xy) * float2(screen_width, screen_height))) + (random(54, gl_GlobalInvocationID) - 0.5f) * 0.2f;

	// Load the parameters of the target pixel
	bool ReflectedRefracted = ReflectedRefractedTex[gl_GlobalInvocationID.xy];
	float depth_curr;
	float3 normal_curr;
	float lum_curr_hf;
	get_shared_data(0, depth_curr, normal_curr, lum_curr_hf, gl_LocalInvocationID);
	Ray ray = CreateCameraRay(ipos / float2(screen_width, screen_height) * 2.0f - 1.0f);
	float4 curprojectedrefl = mul(viewprojection, float4(ray.origin + ray.direction * depth_curr, 1));
	float4 prevprojectedrefl = mul(prevviewprojection, float4(ray.origin + ray.direction * depth_curr, 1));
	float2 reflprojection = ((curprojectedrefl.xy / curprojectedrefl.w) - (prevprojectedrefl.xy / prevprojectedrefl.w)) * 0.5f;


	float motion_length = length((motion.xy + reflprojection) * float2(screen_width, screen_height));


	float2 metal_rough = MetallicA[ipos].xy;
	float shininess = clamp(2.0 / max(pow(metal_rough.y, 4), 0.001f) - 2.0, 0.0, 32.0);

	float3 geo_normal_curr = unpackUnormArb(TEX_PT_NORMALS_A[ipos].x);

	// Try to get the history sample for all channels, including HF moments
	bool temporal_sample_valid_diff = false;
	bool temporal_sample_valid_spec = false;
	SH temporal_color_lf = init_SH();
	float3 temporal_color_hf = 0;
	float4 temporal_moments_histlen_hf = 0;
	float4 temporal_color_histlen_spec = 0;
		float temporal_sum_w_diff = 0.0;
		float temporal_sum_w_spec = 0.0;

		float2 pos_ld = floor(pos_prev - 0.5);
		float2 subpix = frac(pos_prev - 0.5 - pos_ld);
		float MinFrameCount = 999;
	{

		// Bilinear/bilateral filter
		static const int2 off[4] = { { 0, 0 }, { 1, 0 }, { 0, 1 }, { 1, 1 } };
		const float w[4] = {
			(1.0 - subpix.x) * (1.0 - subpix.y),
			(subpix.x) * (1.0 - subpix.y),
			(1.0 - subpix.x) * (subpix.y),
			(subpix.x) * (subpix.y)
		};
		[unroll]for (int i = 0; i < 4; i++) {
			int2 p = int2(pos_ld)+off[i];

			if (p.x < 0 || p.x >= screen_width || p.y >= screen_height)
				continue;

			float depth_prev = TEX_PT_VIEW_DEPTH_B[p].x;
			uint2 norms = TEX_PT_NORMALS_B[p];
			float3  normal_prev = unpackUnormArb(norms.y);
			float3  geo_normal_prev = unpackUnormArb(norms.x);//needs to be TEX_PT_GEO_NORMAL_B, but since for now I am not worrying about normal maps yet, it can use the same texture
			
			bool ReflectedRefractedPrev = ReflectedRefractedTexPrev[p];

			float dist_depth = (abs(depth_curr - depth_prev) - CameraDist) / abs(depth_curr);
			float dot_normals = dot(normal_curr, normal_prev);
			float dot_geo_normals = dot(geo_normal_curr, geo_normal_prev);

			if (depth_curr < 0)
			{
				// Reduce the filter sensitivity to depth for secondary surfaces,
				// because reflection/refracion motion vectors are often inaccurate.
				dist_depth *= 0.25;
			}
			[branch]if(!ReflectedRefracted && !ReflectedRefractedPrev) {
				if (dist_depth < 0.1 && dot_geo_normals > 0.8)
				{
					float w_diff = w[i] * max(dot_normals, 0);
					float w_spec = w[i] * pow(max(dot_normals, 0), shininess);
					MinFrameCount = min(MinFrameCount, TEX_ASVGF_HIST_MOMENTS_HF_B[p].a);

						SH hist_color_lf = load_SH(TEX_ASVGF_HIST_COLOR_LF_SH_B, TEX_ASVGF_HIST_COLOR_LF_COCG_B, p);
						accumulate_SH(temporal_color_lf, hist_color_lf, w_diff);
	

					temporal_color_hf += unpackRGBE(TEX_ASVGF_HIST_COLOR_HF[p]) * w_diff;
					temporal_color_histlen_spec += float4(unpackRGBE(asuint(TEX_ASVGF_FILTERED_SPEC_B[p].x)), TEX_ASVGF_FILTERED_SPEC_B[p].y) * w_spec;
					temporal_moments_histlen_hf += TEX_ASVGF_HIST_MOMENTS_HF_B[p].rgba * w_diff;
					temporal_sum_w_diff += w_diff;
					temporal_sum_w_spec += w_spec;
				}
			} else {
				if (dist_depth < 0.1)
				{
					float w_diff = w[i];
					float w_spec = w[i];

					SH hist_color_lf = load_SH(TEX_ASVGF_HIST_COLOR_LF_SH_B, TEX_ASVGF_HIST_COLOR_LF_COCG_B, p);
					accumulate_SH(temporal_color_lf, hist_color_lf, w_diff);

					temporal_color_hf += unpackRGBE(TEX_ASVGF_HIST_COLOR_HF[p]) * w_diff;
					temporal_color_histlen_spec += float4(unpackRGBE(asuint(TEX_ASVGF_FILTERED_SPEC_B[p].x)), TEX_ASVGF_FILTERED_SPEC_B[p].y) * w_spec;
					temporal_moments_histlen_hf += TEX_ASVGF_HIST_MOMENTS_HF_B[p].rgba * w_diff;
					temporal_sum_w_diff += w_diff;
					temporal_sum_w_spec += w_spec;
				}	
			}
		}

		// We found some relevant surfaces - good
		if (temporal_sum_w_diff > 1e-6)
		{
			float inv_w_diff = 1.0 / temporal_sum_w_diff;
			temporal_color_lf.shY *= inv_w_diff;
			temporal_color_lf.CoCg *= inv_w_diff;
			temporal_color_hf *= inv_w_diff;
			temporal_moments_histlen_hf *= inv_w_diff;
			temporal_sample_valid_diff = true;
		}

		if (temporal_sum_w_spec > 1e-6)
		{
			float inv_w_spec = 1.0 / temporal_sum_w_spec;
			temporal_color_histlen_spec *= inv_w_spec;
			temporal_sample_valid_spec = true;
		}

	}


	// Compute spatial moments of the HF channel in a 3x3 window
	float2 spatial_moments_hf = float2(lum_curr_hf, lum_curr_hf * lum_curr_hf);

	{
		float spatial_sum_w_hf = 1.0;
		for (int yy = -FILTER_RADIUS; yy <= FILTER_RADIUS; yy++) {
			for (int xx = -FILTER_RADIUS; xx <= FILTER_RADIUS; xx++) {
				if (xx == 0 && yy == 0)
					continue;

				int2 p = ipos + int2(xx, yy);

				float depth;
				float3 normal;
				float lum_p_hf;
				get_shared_data(int2(xx, yy), depth, normal, lum_p_hf, gl_LocalInvocationID);
				// lum_p_hf += luminance(unpackRGBE(TEX_PT_COLOR_SPEC[p].x));
				float dist_z = abs(depth_curr - depth) / abs(depth_curr);
				if (dist_z < 2.0) {
					float w_hf = 1;//pow(max(0.0, dot(normal, normal_curr)), 128.0);

					spatial_moments_hf += float2(lum_p_hf * w_hf, lum_p_hf * lum_p_hf * w_hf);
					spatial_sum_w_hf += w_hf;
				}
			}
		}

		float inv_w2_hf = 1.0f / spatial_sum_w_hf;
		spatial_moments_hf *= inv_w2_hf;
	}

	// Load the target pixel colors for all channels
	SH color_curr_lf = load_SH(TEX_PT_COLOR_LF_SH, TEX_PT_COLOR_LF_COCG, ipos);
	float3 color_curr_hf = unpackRGBE(TEX_PT_COLOR_HF[ipos]);
	float3 color_curr_spec = unpackRGBE(TEX_PT_COLOR_SPEC[ipos]);

	SH out_color_lf;
	float3 out_color_hf;
	float4 out_color_histlen_spec;
	float4 out_moments_histlen_hf;

	// Load the gradients
	float grad_lf = TEX_ASVGF_GRAD_LF_PONG[ipos / GRAD_DWN].r;
	float2 grad_hf_spec = TEX_ASVGF_GRAD_HF_SPEC_PONG[ipos / GRAD_DWN].rg;
	grad_lf = clamp(grad_lf, 0, 1);
	grad_hf_spec = clamp(grad_hf_spec, 0, 1);

	if (temporal_sample_valid_diff)
	{
		// Compute the antilag factors based on the gradients
		float antilag_alpha_lf = clamp(lerp(1.0, 0.2f * grad_lf, 1.0f), 0, 1);//play with the middle 2 values?
		float antilag_alpha_hf = clamp(lerp(1.0, 1.0f * grad_hf_spec.x, 1.0f), 0, 1);//play with the middle 2 values?

		// Adjust the history length, taking the antilag factors into account
		float hist_len_hf = min(temporal_moments_histlen_hf.b * pow(1.0 - antilag_alpha_hf, 10) + 1.0, 256.0);
		float hist_len_lf = min(temporal_moments_histlen_hf.a * pow(1.0 - antilag_alpha_lf, 10) + 1.0, 256.0);

		// Compute the blending weights based on history length, so that the filter
		// converges faster. I.e. the first frame has weight of 1.0, the second frame 1/2, third 1/3 and so on.
		float alpha_color_lf = max(0.01f, 1.0f / (hist_len_lf));
		float alpha_color_hf = max(0.02f, 1.0f / (hist_len_hf));
		float alpha_moments_hf = max(0.01f, 1.0f / (hist_len_hf));

		// Adjust the blending factors, taking the antilag factors into account again
		alpha_color_lf = lerp(alpha_color_lf, 1.0, antilag_alpha_lf);
		alpha_color_hf = lerp(alpha_color_hf, 1.0, antilag_alpha_hf);
		alpha_moments_hf = lerp(alpha_moments_hf, 1.0, antilag_alpha_hf);

		// Blend!
		out_color_lf = mix_SH(temporal_color_lf, color_curr_lf, alpha_color_lf);
		out_color_hf.rgb = lerp(temporal_color_hf.rgb, color_curr_hf.rgb, alpha_color_hf);

		out_moments_histlen_hf.rg = lerp(temporal_moments_histlen_hf.rg, spatial_moments_hf.rg, alpha_moments_hf);
		out_moments_histlen_hf.b = hist_len_hf;
		out_moments_histlen_hf.a = hist_len_lf;
	}
	else
	{
		// No valid history - just use the current color and spatial moments
		out_color_lf = color_curr_lf;
		out_color_hf.rgb = color_curr_hf;
		out_moments_histlen_hf = float4(spatial_moments_hf, 1, 1);
	}

	if (temporal_sample_valid_spec)
	{
		float3 Max = 0;
		float3 Min = 9999;
		for(int x = -1; x <= 1; x++) {
			for(int y = -1; y <= 1; y++) {
				if(x == 0 && y == 0) continue;
				float3 col = unpackRGBE(TEX_PT_COLOR_SPEC[ipos + int2(x,y)]);
				Max = max(Max, col);
				Min = min(Min, col);
			}
		}
		color_curr_spec.rgb = exp(clamp(log(color_curr_spec.rgb + 1), 0, log(Max + 15))) - 1;

		// Same sequence as above, only for the specular channel
		float antilag = grad_hf_spec.y * clamp(1.0f - motion_length, 0.1f, 1.0f) + motion_length * 0.001f;
		float antilag_alpha_spec = clamp(lerp(0.0, antilag, 1), 0, 1);
		float hist_len_spec = min(temporal_color_histlen_spec.a * pow(1.0 - antilag_alpha_spec, 10) + 2.0, 256.0);
		float alpha_color_spec = max(0.01f, 1.0 / hist_len_spec);
		alpha_color_spec = lerp(alpha_color_spec, 1.0, antilag_alpha_spec);
		out_color_histlen_spec.rgb = lerp(temporal_color_histlen_spec.rgb, color_curr_spec.rgb, alpha_color_spec);
		out_color_histlen_spec.a = hist_len_spec;
	}
	else
	{
		out_color_histlen_spec = float4(color_curr_spec, 1);
	}


	// Store the outputs for furhter processing by the a-trous HF filter
	IMG_ASVGF_HIST_MOMENTS_HF_A[ipos] = out_moments_histlen_hf;
	// DebugTex[ipos] = out_color_lf.shY.w;
	STORE_SH(IMG_ASVGF_HIST_COLOR_LF_SH_A, IMG_ASVGF_HIST_COLOR_LF_COCG_A, ipos, out_color_lf);
	IMG_ASVGF_ATROUS_PING_HF[ipos] = packRGBE(out_color_hf);
	IMG_ASVGF_ATROUS_PING_SPEC[ipos] = float2(asfloat(packRGBE(out_color_histlen_spec)), out_color_histlen_spec.a);
	IMG_ASVGF_ATROUS_PING_MOMENTS[ipos] = out_moments_histlen_hf.xy;

	GroupMemoryBarrierWithGroupSync();

	// Store the LF channel into shared memory for averaging
	s_lf_shy[gl_LocalInvocationID.y][gl_LocalInvocationID.x] = out_color_lf.shY;
	s_lf_cocg[gl_LocalInvocationID.y][gl_LocalInvocationID.x] = out_color_lf.CoCg;

	if (gl_LocalInvocationID.x % GRAD_DWN == 1 && gl_LocalInvocationID.y % GRAD_DWN == 1)
	{
		s_depth_width[gl_LocalInvocationID.y / GRAD_DWN][gl_LocalInvocationID.x / GRAD_DWN] = FDepth[ipos];
	}
	GroupMemoryBarrierWithGroupSync();

	// Comptue a 1/3-resolution version of the LF channel for this group.
	// Use a bilateral filter that takes the center pixel of each 3x3 square as the anchor.

	float2 lowres_local_id;
	lowres_local_id.x = gl_LocalInvocationIndex % (GROUP_SIZE / GRAD_DWN);
	lowres_local_id.y = gl_LocalInvocationIndex / (GROUP_SIZE / GRAD_DWN);

	if (lowres_local_id.y >= (GROUP_SIZE / GRAD_DWN))
		return;

	// Load the anchor pixel info
	float2 center_shared_pos = lowres_local_id * GRAD_DWN + 1;
	float3 center_normal = s_normal_lum[center_shared_pos.y + FILTER_RADIUS][center_shared_pos.x + FILTER_RADIUS].xyz;
	float center_depth = s_depth[center_shared_pos.y + FILTER_RADIUS][center_shared_pos.x + FILTER_RADIUS];
	float depth_width = s_depth_width[lowres_local_id.y][lowres_local_id.x];

	SH center_lf;
	center_lf.shY = s_lf_shy[center_shared_pos.y][center_shared_pos.x];
	center_lf.CoCg = s_lf_cocg[center_shared_pos.y][center_shared_pos.x];

	float sum_w = 1;
	SH sum_lf = center_lf;

	// Average the anchor pixel color with the relevant neighborhood
	for (int yy = -1; yy <= 1; yy++)
	{
		for (int xx = -1; xx <= 1; xx++)
		{
			if (yy == 0 && xx == 0)
				continue;

			float3 p_normal = s_normal_lum[center_shared_pos.y + FILTER_RADIUS + yy][center_shared_pos.x + FILTER_RADIUS + xx].xyz;
			float p_depth = s_depth[center_shared_pos.y + FILTER_RADIUS + yy][center_shared_pos.x + FILTER_RADIUS + xx];

			float dist_depth = abs(p_depth - center_depth) * depth_width;
			if (dist_depth < 2)
			{
				float w = 1;//pow(max(dot(p_normal, center_normal), 0), 8);

				SH p_lf;
				p_lf.shY = s_lf_shy[center_shared_pos.y + yy][center_shared_pos.x + xx];
				p_lf.CoCg = s_lf_cocg[center_shared_pos.y + yy][center_shared_pos.x + xx];

				accumulate_SH(sum_lf, p_lf, w);
				sum_w += w;
			}
		}
	}

	float inv_w = 1.0f / (sum_w);
	sum_lf.shY *= inv_w;
	sum_lf.CoCg *= inv_w;

	// Store the LF result for further processing by the a-trous LF filter
	int2 ipos_lowres = int2(gl_WorkGroupID.xy) * (GROUP_SIZE / GRAD_DWN) + int2(lowres_local_id);
	STORE_SH(IMG_ASVGF_ATROUS_PING_LF_SH, IMG_ASVGF_ATROUS_PING_LF_COCG, ipos_lowres, sum_lf);
	// if((ipos_lowres * GRAD_DWN).y > screen_height / 2) {
	// 	for(int i = 0; i < GRAD_DWN; i++) {
	// 		for(int j = 0; j < GRAD_DWN; j++) {
	// 			STORE_SH(IMG_ASVGF_HIST_COLOR_LF_SH_A, IMG_ASVGF_HIST_COLOR_LF_COCG_A, ipos_lowres * GRAD_DWN + int2(i, j), sum_lf);
	// 		}
	// 	}
	// }
}



#pragma kernel Atrous_LF

int MaxIterations;

uniform bool UseASVGF;
Texture2D<float> LFVarianceA;
RWTexture2D<float> LFVarianceB;

inline float3 project_SH_irradiance(SH sh, float3 N)
{
	float d = dot(sh.shY.xyz, N);
	float Y = 2.0 * (1.023326 * d + 0.886226 * sh.shY.w);
	Y = max(Y, 0.0);

	sh.CoCg *= Y * 0.282095 / (sh.shY.w + 1e-6);

	float   T = Y - sh.CoCg.y * 0.5;
	float   G = sh.CoCg.y + T;
	float   B = T - sh.CoCg.x * 0.5;
	float   R = B + sh.CoCg.x;

	return max(exp(float3(R, G, B)) - 1, 0.0);
}


inline void filter_image(
	Texture2D<float4> img_lf_shY,
	Texture2D<float2> img_lf_CoCg,
	out SH filtered_lf, int2 gl_GlobalInvocationID)
{
	int2 ipos_lowres = gl_GlobalInvocationID;
	int2 ipos_hires = ipos_lowres * GRAD_DWN + 1;

	// Load the color of the target low-res pixel
	SH color_center_lf = load_SH(img_lf_shY, img_lf_CoCg, ipos_lowres);

	if (5 <= iteration)
	{
		filtered_lf = color_center_lf;
		return;
	}

	// Load the parameters of the anchor pixel
	float3 geo_normal_center = unpackUnormArb(TEX_PT_NORMALS_A[ipos_hires].x);
	float depth_center = TEX_PT_VIEW_DEPTH_A[ipos_hires].x;
	float fwidth_depth = FDepth[ipos_hires];

	float step_size = int(1u << (iteration));
	if(iteration == 4) step_size = int(1u << (1));
	SH sum_color_lf = color_center_lf;

	float sum_w_lf = 1.0;
	float hist_len_lf = TEX_ASVGF_HIST_MOMENTS_HF_A[ipos_hires].a;
	int2 jitter = int2(step_size * 0.5f * (random(23, gl_GlobalInvocationID.xy) - 0.5));
	step_size *= clamp(step_size * (LFVarianceA[ipos_lowres] / (1.0f + (hist_len_lf / 8.0f)) + (1.0f - min(hist_len_lf / 6.0f,1))), 1.0f, step_size * 2.0f);
	step_size *= clamp(2.0 * ((1.0f - min(hist_len_lf / 12.0f,1))), 1.0f, 2.0f);
	float TotVariance = LFVarianceA[ipos_lowres];/// (1.0f + (hist_len_lf / 8.0f));
	// Compute the weighted average of color and moments from a sparse 3x3 pattern around the target pixel
	const int r = 1;
	for (int yy = -r; yy <= r; yy++) {
		for (int xx = -r; xx <= r; xx++) {
			int2 p_lowres = ipos_lowres + int2(xx, yy) * step_size + jitter;
			int2 p_hires = p_lowres * GRAD_DWN + 1;


			if (xx == 0 && yy == 0)
				continue;

			float w = float(all((p_hires >= int2(0, 0)))
				&& all((p_hires < int2(screen_width, screen_height))));
			// Use geometric normals here so that we can blur over larger areas.
			// The lighting detail will be partially preserved by spherical harmonics.
			float3 geo_normal = unpackUnormArb(TEX_PT_NORMALS_A[p_hires].x);

			float depth = TEX_PT_VIEW_DEPTH_A[p_hires].x;

			float dist_z = abs(depth_center - depth);// * fwidth_depth;
			w *= exp(-dist_z);
			w *= wavelet_kernel[abs(xx)][abs(yy)];
			// w *= (2.0f - (1 + LFVarianceA[p_lowres]));


			float w_lf = w;

			SH c_lf = load_SH(img_lf_shY, img_lf_CoCg, p_lowres);

			float Weight1 = max(luminance(project_SH_irradiance(load_SH(img_lf_shY, img_lf_CoCg, ipos_lowres), geo_normal_center)) * 1024.0f* 1024.0f,0);
			float Weight2 = max(luminance(project_SH_irradiance(load_SH(img_lf_shY, img_lf_CoCg, p_lowres), geo_normal)) * 1024.0f* 1024.0f,0);

			float GNdotGN = max(0.0, dot(geo_normal_center, geo_normal));
			w_lf *= pow(GNdotGN, 8);
			float w_l = clamp(exp(-pow(abs(Weight2 - Weight1) / (1.0f + (Weight1)),1) * max(1.0f - LFVarianceA[p_lowres], 0)  * clamp(pow(hist_len_lf / 32.0f,2), 0, 1)),0.1f, 1.0f);
			if ((Weight1 + Weight2) / 2048.0f > 0.01f && iteration >= 1 && iteration < 5)
				w_lf *= w_l;
			// if(w_lf < 0.1f) continue;

			// The 4th iteration has filter footprint big enough to step over obstacles and produce noticeable light leaking.
			// Prevent that by throwing away samples that are too bright. This also helps make some shadows a bit sharper.
			if (iteration >= 3 && iteration < 4)
				w_lf *= clamp(1.5 - Weight2 / Weight1 * 0.25, 0, 1);
			TotVariance += LFVarianceA[p_lowres] * w_lf;
			accumulate_SH(sum_color_lf, c_lf, w_lf);
			sum_w_lf += w_lf;
		}
	}
	filtered_lf.shY = sum_color_lf.shY / sum_w_lf;
	filtered_lf.CoCg = sum_color_lf.CoCg / sum_w_lf;
	LFVarianceB[ipos_lowres] = TotVariance / sum_w_lf;
}

inline void deflicker_image(
	Texture2D<float4> img_lf_shY,
	Texture2D<float2> img_lf_CoCg,
	out SH filtered_lf, int2 gl_GlobalInvocationID)
{
	int2 ipos_lowres = gl_GlobalInvocationID;
	SH color_center_lf = load_SH(img_lf_shY, img_lf_CoCg, ipos_lowres);
	int2 ipos_hires = ipos_lowres * GRAD_DWN + 1;

	SH sum_color_lf = init_SH();

	const int r = 1;
	const float num_pixels = pow(r * 2 + 1, 2) - 1;
	float MaxLum = 0;
	float MinLum = 9999.0f;
	for (int yy = -r; yy <= r; yy++) {
		for (int xx = -r; xx <= r; xx++) {
			int2 p_lowres = ipos_lowres + int2(xx, yy);

			if (xx == 0 && yy == 0)
				continue;

			SH c_lf = load_SH(img_lf_shY, img_lf_CoCg, p_lowres);
			MaxLum = max(MaxLum, c_lf.shY.w);
			MinLum = min(MinLum, c_lf.shY.w);
			accumulate_SH(sum_color_lf, c_lf, 1.0);
		}
	}


	float max_lum = sum_color_lf.shY.w * 2 / num_pixels;
	if (color_center_lf.shY.w > max_lum)
	{
		float ratio = max_lum / color_center_lf.shY.w;
		float ratio2 = (exp(max_lum) - 1) * 1024.0f * 1024.0f / ((exp(color_center_lf.shY.w) - 1) * 1024.0f * 1024.0f);
		// if(TEX_ASVGF_HIST_MOMENTS_HF_A[ipos_hires].a > 4) {
			color_center_lf.shY *= ratio;
			color_center_lf.CoCg *= ratio;
		// }
		LFVarianceB[gl_GlobalInvocationID.xy] = ratio;
	} else {
		LFVarianceB[gl_GlobalInvocationID.xy] = 0.1f;
	}


	{	
		// float ratio = color_center_lf.shY.w / (clamp(color_center_lf.shY.w / 0.282095, MinLum / 0.282095 + 1, MaxLum / 0.282095 + 1) * 0.282095);
		// if(ratio == 0) ratio = 1;
		// color_center_lf.shY *= (ratio == 0 ? 0 : rcp(ratio));
		// color_center_lf.CoCg *= (ratio == 0 ? 0 : rcp(ratio));
		// LFVarianceB[gl_GlobalInvocationID.xy] *= rcp(ratio);
	}

	filtered_lf = color_center_lf;
}

Texture2D<float4> TEX_ASVGF_ATROUS_PING_LF_SH;
Texture2D<float2> TEX_ASVGF_ATROUS_PING_LF_COCG;
RWTexture2D<float4> IMG_ASVGF_ATROUS_PONG_LF_SH;
RWTexture2D<float2> IMG_ASVGF_ATROUS_PONG_LF_COCG;


[numthreads(16, 16, 1)]
void Atrous_LF(uint3 gl_GlobalInvocationID : SV_DispatchThreadID, uint3 gl_WorkGroupID : SV_GroupID, uint gl_LocalInvocationIndex : SV_GroupIndex, uint3 gl_LocalInvocationID : SV_GroupThreadID)
{

	int2 ipos = gl_GlobalInvocationID;
	if (any((ipos * GRAD_DWN >= int2(screen_width, screen_height))))
		return;

	SH filtered_lf;

	[branch] switch (iteration) {
	case 0: deflicker_image(TEX_ASVGF_ATROUS_PING_LF_SH, TEX_ASVGF_ATROUS_PING_LF_COCG, filtered_lf, ipos); break;
	case 1: filter_image(TEX_ASVGF_ATROUS_PONG_LF_SH, TEX_ASVGF_ATROUS_PONG_LF_COCG, filtered_lf, ipos); break;
	case 2: filter_image(TEX_ASVGF_ATROUS_PING_LF_SH, TEX_ASVGF_ATROUS_PING_LF_COCG, filtered_lf, ipos); break;
	case 3: filter_image(TEX_ASVGF_ATROUS_PONG_LF_SH, TEX_ASVGF_ATROUS_PONG_LF_COCG, filtered_lf, ipos); break;
	case 4: filter_image(TEX_ASVGF_ATROUS_PING_LF_SH, TEX_ASVGF_ATROUS_PING_LF_COCG, filtered_lf, ipos); break;
	}

	// if(iteration == 0) DebugTex[ipos] = filtered_lf.shY;

	[branch] switch (iteration) {
	case 0: STORE_SH(IMG_ASVGF_ATROUS_PONG_LF_SH, IMG_ASVGF_ATROUS_PONG_LF_COCG, ipos, filtered_lf); break;
	case 1: STORE_SH(IMG_ASVGF_ATROUS_PING_LF_SH, IMG_ASVGF_ATROUS_PING_LF_COCG, ipos, filtered_lf); break;
	case 2: STORE_SH(IMG_ASVGF_ATROUS_PONG_LF_SH, IMG_ASVGF_ATROUS_PONG_LF_COCG, ipos, filtered_lf); break;
	case 3: STORE_SH(IMG_ASVGF_ATROUS_PING_LF_SH, IMG_ASVGF_ATROUS_PING_LF_COCG, ipos, filtered_lf); break;
	case 4: STORE_SH(IMG_ASVGF_ATROUS_PONG_LF_SH, IMG_ASVGF_ATROUS_PONG_LF_COCG, ipos, filtered_lf); break;
	}
	// DebugTex[ipos] = LFVarianceB[ipos];
}


#pragma kernel Atrous



uniform int spec_iteration;

float square(float x) { return x * x; }

// Converts a square of roughness to a Phong specular power
float RoughnessSquareToSpecPower(in float alpha) {
	return max(0.01, 2.0f / (square(alpha) + 1e-4) - 2.0f);
}

inline void filter_image(
	Texture2D<uint> img_hf,
	Texture2D<float2> img_spec,
	Texture2D<float2> img_moments,
	out float3 filtered_hf,
	out float3 filtered_spec,
	out float2 filtered_moments,
	int2 gl_GlobalInvocationID)
{
	int2 ipos = int2(gl_GlobalInvocationID);

	float3 color_center_hf = unpackRGBE(img_hf[ipos]);
	float3 color_center_spec = unpackRGBE(asuint(img_spec[ipos].x));
	float2 moments_center = img_moments[ipos].xy;

	if (3 <= spec_iteration)
	{
		filtered_hf = color_center_hf;
		filtered_spec = color_center_spec;
		filtered_moments = moments_center;
		return;
	}

	float3 normal_center = unpackUnormArb(TEX_PT_NORMALS_A[ipos].y);
	float depth_center = TEX_PT_VIEW_DEPTH_A[ipos].x;
	float fwidth_depth = FDepth[ipos];
	float roughness_center = MetallicA[ipos].y;

	float lum_mean_hf = 0;
	float sigma_l_hf = 0;

	float hist_len_hf = TEX_ASVGF_HIST_MOMENTS_HF_A[ipos].b;

	if (hist_len_hf > 1)
	{
		// Compute luminance variance from the statistical moments: Var(X) = E[X^2] - E[X]^2
		// The `asvgf_temporal` shader computes a combination of temporal and spatial (3x3) moments,
		// and stores these into a texture. This shader will combine moments of the surrounding 
		// pixels using the same weights as for colors, and the combined moments are used on the next iteration.
		lum_mean_hf = moments_center.x;
		float lum_variance_hf = max(1e-8, moments_center.y - moments_center.x * moments_center.x);
		sigma_l_hf = min(hist_len_hf, 16) / (2.0 * lum_variance_hf);
	}
	else
	{
		// If there is no history, treat all moments as invalid, because 3x3 spatial 
		// is just not enough to get reasonable filtering. Ignore luminance in this case,
		// and perform a depth-normal-guided bilateral blur.
		sigma_l_hf = 0;
	}

	// reduce the HF filter sensitivity to normals when the lighting is invalidated
	float normal_weight_scale = clamp(hist_len_hf / 8, 0, 1);

	float normal_weight_hf = 64;
	normal_weight_hf *= normal_weight_scale;
	float normal_weight_spec = RoughnessSquareToSpecPower(square(roughness_center)) * 1;
	normal_weight_spec = clamp(normal_weight_spec, 8, 1024);
	normal_weight_spec *= normal_weight_scale;

	int step_size = int(1u << spec_iteration);
	step_size *= clamp(step_size * (max(1e-8, moments_center.x * moments_center.x * 10.0f)), 1.0f, step_size);

	float3 sum_color_hf = color_center_hf.rgb;
	float3 sum_color_spec = color_center_spec.rgb;
	float2 sum_moments = moments_center;

	float sum_w_hf = 1.0;
	float sum_w_spec = 1.0;

	// Add some jitter to sample positions to hide the a-trous filter aliasing patterns
	int2 jitter = int2((random(spec_iteration, gl_GlobalInvocationID.xy) - 0.5) * float(step_size));

	float spec_filter_width_scale = clamp(roughness_center * 30 - spec_iteration, 0, 1);


	// Compute the weighted average of color and moments from a sparse 3x3 pattern around the target pixel
	const int r = 1;
	for (int yy = -r; yy <= r; yy++) {
		for (int xx = -r; xx <= r; xx++) {
			int2 p = ipos + int2(xx, yy) * step_size + jitter;

			// int actxx = xx;
			// int actyy = yy;
			// if(p.x <= 0 || p.x > screen_width) {
			// 	actxx *= -1;
			// }
			// if(p.y <= 0 || p.y > screen_height) {
			// 	actyy *= -1;
			// }

			// p = ipos + int2(xx, yy) * step_size + jitter;
			if (xx == 0 && yy == 0)
				continue;

			float w = float(all((p >= 0))
				&& all((p < int2(screen_width, screen_height))));

			float3 normal = unpackUnormArb(TEX_PT_NORMALS_A[p].y);

			float depth = TEX_PT_VIEW_DEPTH_A[p].x;
			float roughness = MetallicA[p].y;

			float dist_z = abs(depth_center - depth) * fwidth_depth;
			w *= exp(-dist_z / float(step_size));
			w *= wavelet_kernel[abs(xx)][abs(yy)];

			float w_hf = w;

			float3 c_hf = unpackRGBE(img_hf[p]);
			float3 c_spec = unpackRGBE(asuint(img_spec[p].x));
			float2 c_mom = img_moments[p].xy;
			float l_hf = luminance(c_hf.rgb);
			float dist_l_hf = abs(lum_mean_hf - l_hf);

			w_hf *= exp(-dist_l_hf * dist_l_hf * sigma_l_hf);

			float w_spec = w_hf;
			// w_spec *= max(0, 1 - 10 * abs(roughness - roughness_center));
			w_spec *= spec_filter_width_scale;

			// w_hf *= 1.0f - clamp(abs(MetallicA[p].x - MetallicA[ipos].x),0,1);

			float NdotN = max(0.0, dot(normal_center, normal));

			if (normal_weight_hf > 0)
			{
				w_hf *= pow(NdotN, normal_weight_hf);
			}

			if (normal_weight_spec > 0)
			{
				// w_spec *= pow(NdotN, normal_weight_spec);
			}


			if (MaxIterations <= spec_iteration)
				w_hf = 0;


			if (MaxIterations <= spec_iteration)
				w_spec = 0;


			sum_color_hf += c_hf.rgb * w_hf;
			sum_color_spec += c_spec.rgb * w_spec;
			sum_moments += c_mom * w_hf;
			sum_w_hf += w_hf;
			sum_w_spec += w_spec;
		}
	}

	filtered_hf = sum_color_hf / sum_w_hf;
	filtered_spec = sum_color_spec / sum_w_spec;
	filtered_moments = sum_moments / sum_w_hf;
}

inline SH interpolate_lf(Texture2D<float4> img_lf_shY, Texture2D<float2> img_lf_CoCg, int2 ipos)
{
	// Target pixel parameters
	float depth_center = TEX_PT_VIEW_DEPTH_A[ipos].x;
	float fwidth_depth = FDepth[ipos];
	float3 geo_normal_center = unpackUnormArb(TEX_PT_NORMALS_A[ipos].x);


	float2 pos_lowres = (float2(ipos)+0.5) / GRAD_DWN - 0.5;
	float2 pos_ld = floor(pos_lowres);
	float2 subpix = frac(pos_lowres - pos_ld);

	SH sum_lf = init_SH();
	float sum_w = 0;

	// 4 bilinear taps
	const int2 off[4] = { { 0, 0 }, { 1, 0 }, { 0, 1 }, { 1, 1 } };
	float w[4] = {
		(1.0 - subpix.x) * (1.0 - subpix.y),
		(subpix.x) * (1.0 - subpix.y),
		(1.0 - subpix.x) * (subpix.y),
		(subpix.x) * (subpix.y)
	};
	for (int i = 0; i < 4; i++)
	{
		int2 p_lowres = int2(pos_ld)+off[i];
		int2 p_hires = p_lowres * GRAD_DWN + 1;

		// Low-res pixel parameters
		float p_depth = TEX_PT_VIEW_DEPTH_A[p_hires].x;
		float3 p_geo_normal = unpackUnormArb(TEX_PT_NORMALS_A[p_hires].x);

		// Start with bilinear weight
		float p_w = w[i];

		// Compute depth and normal similarity between the target pixel and the low-res anchor pixel
		float dist_depth = abs(p_depth - depth_center) * fwidth_depth * 1.0f;
		p_w *= exp(-dist_depth);
		p_w *= pow(max(0.0, dot(geo_normal_center, p_geo_normal)), 8);

		if (p_w > 0)
		{
			SH p_lf = load_SH(img_lf_shY, img_lf_CoCg, p_lowres);
			accumulate_SH(sum_lf, p_lf, p_w);
			sum_w += p_w;
		}
	}

	if (sum_w > 0)
	{
		// We found at least one relevant pixel among the 4 bilinear taps - good
		float inv_w = 1.0f / (sum_w);
		sum_lf.shY *= inv_w;
		sum_lf.CoCg *= inv_w;
	}
	else
	{
		// We didn't find anything relevant, so use the full-res temporally filtered LF data instead
		sum_lf = load_SH(TEX_ASVGF_HIST_COLOR_LF_SH_A, TEX_ASVGF_HIST_COLOR_LF_COCG_A, ipos);
	}

	return sum_lf;
}

bool DiffRes;

#ifdef HDRP
	Texture2DArray<float4> DiffuseGBuffer;
	Texture2DArray<float4> SpecularGBuffer;
#else
	Texture2D<float4> DiffuseGBuffer;
	Texture2D<float4> SpecularGBuffer;
#endif

float3 composite_color(float4 surf_base_color,
	float3 projected_lf, float3 high_freq, float3 specular, float3 SpecColor, int2 ipos)
{
	projected_lf *= 1.0f;
	high_freq *= 1;
	specular *= 1;
	float3 GBufferCol = 1;
	// if(DiffRes) {
	// 	#ifdef HDRP
	//     	float3 SpecularAlbedo = 0;//Albedo2[int3(ipos,0)].xyz;
	// 		GBufferCol = 1;//((DiffuseGBuffer.SampleLevel(my_linear_clamp_sampler, float3(ipos / float2(screen_width, screen_height), 0), 0).xyz + SpecularAlbedo) == 0) ? 1 : (DiffuseGBuffer.SampleLevel(my_linear_clamp_sampler, float3(ipos / float2(screen_width, screen_height), 0), 0).xyz + SpecularAlbedo);
	// 	#else
	// 		float3 SpecularAlbedo = 1;//SpecularGBuffer.SampleLevel(my_linear_clamp_sampler, ipos / float2(screen_width, screen_height), 0);
	// 		GBufferCol = 1;//((DiffuseGBuffer.SampleLevel(my_linear_clamp_sampler, ipos / float2(screen_width, screen_height), 0).xyz + SpecularAlbedo) == 0) ? 1 : ((DiffuseGBuffer.SampleLevel(my_linear_clamp_sampler, ipos / float2(screen_width, screen_height), 0).xyz + SpecularAlbedo));
	// 	#endif
	// }
	float3 final_color = ((surf_base_color.w == 0) ? pow(surf_base_color, 1.0f) : ((projected_lf.rgb + high_freq.rgb) * surf_base_color + specular * SpecColor));

	return final_color;
}

Texture2D<float2> TEX_ASVGF_ATROUS_PING_MOMENTS;
Texture2D<float2> TEX_ASVGF_ATROUS_PONG_SPEC;
Texture2D<float2> TEX_ASVGF_ATROUS_PING_SPEC;
Texture2D<float2> TEX_ASVGF_ATROUS_PONG_MOMENTS;
Texture2D<uint> TEX_ASVGF_ATROUS_PONG_HF;
RWTexture2D<uint> IMG_ASVGF_HIST_COLOR_HF;
RWTexture2D<float2> IMG_ASVGF_ATROUS_PONG_SPEC;
RWTexture2D<float2> IMG_ASVGF_ATROUS_PONG_MOMENTS;
RWTexture2D<uint> IMG_ASVGF_ATROUS_PONG_HF;
RWTexture2D<float4> IMG_ASVGF_COLOR;

[numthreads(16, 16, 1)]
void Atrous(uint3 gl_GlobalInvocationID : SV_DispatchThreadID, uint3 gl_WorkGroupID : SV_GroupID, uint gl_LocalInvocationIndex : SV_GroupIndex, uint3 gl_LocalInvocationID : SV_GroupThreadID)
{
	int2 ipos = gl_GlobalInvocationID.xy;
	if (any((ipos >= int2(screen_width, screen_height))))
		return;

	float3 filtered_hf;
	float3 filtered_spec;
	float2 filtered_moments;

	[branch] switch (spec_iteration) {
	case 0: filter_image(TEX_ASVGF_ATROUS_PING_HF, TEX_ASVGF_ATROUS_PING_SPEC, TEX_ASVGF_ATROUS_PING_MOMENTS, filtered_hf, filtered_spec, filtered_moments, ipos); break;
	case 1: filter_image(TEX_ASVGF_HIST_COLOR_HF, TEX_ASVGF_ATROUS_PONG_SPEC, TEX_ASVGF_ATROUS_PONG_MOMENTS, filtered_hf, filtered_spec, filtered_moments, ipos); break;
	case 2: filter_image(TEX_ASVGF_ATROUS_PING_HF, TEX_ASVGF_ATROUS_PING_SPEC, TEX_ASVGF_ATROUS_PING_MOMENTS, filtered_hf, filtered_spec, filtered_moments, ipos); break;
	case 3: filter_image(TEX_ASVGF_ATROUS_PONG_HF, TEX_ASVGF_ATROUS_PONG_SPEC, TEX_ASVGF_ATROUS_PONG_MOMENTS, filtered_hf, filtered_spec, filtered_moments, ipos); break;
	}

	[branch] switch (spec_iteration) {
	case 0:
		// DebugTex[ipos] = TEX_ASVGF_ATROUS_PING_HF[ipos];//float4(filtered_hf, 0);
		IMG_ASVGF_HIST_COLOR_HF[ipos] = packRGBE(filtered_hf);
		IMG_ASVGF_ATROUS_PONG_SPEC[ipos] = float2(asfloat(packRGBE(filtered_spec)),0);
		IMG_ASVGF_ATROUS_PONG_MOMENTS[ipos] = filtered_moments;
		break;
	case 1:
		IMG_ASVGF_ATROUS_PING_HF[ipos] = packRGBE(filtered_hf);
		IMG_ASVGF_ATROUS_PING_SPEC[ipos] = float2(asfloat(packRGBE(filtered_spec)),0);
		IMG_ASVGF_ATROUS_PING_MOMENTS[ipos] = filtered_moments;
		break;
	case 2:
		IMG_ASVGF_ATROUS_PONG_HF[ipos] = packRGBE(filtered_hf);
		IMG_ASVGF_ATROUS_PONG_SPEC[ipos] = float2(asfloat(packRGBE(filtered_spec)),0);
		IMG_ASVGF_ATROUS_PONG_MOMENTS[ipos] = filtered_moments;
		break;
	}

	// Perform compositing on the last iteration
	if (spec_iteration == MaxIterations - 1)
	{
		SH filtered_lf = interpolate_lf(TEX_ASVGF_ATROUS_PING_LF_SH, TEX_ASVGF_ATROUS_PING_LF_COCG, ipos);


		float3 normal = unpackUnormArb(TEX_PT_NORMALS_A[ipos].y);
		float4 base_color = float4(unpackRGBE(AlbedoColorB[ipos].x),TEX_PT_BASE_COLOR_A[ipos].w);// TEX_PT_BASE_COLOR_A.SampleLevel(sampler_trilinear_clamp, ipos / float2(screen_width, screen_height), 0);
		if(DiffRes && base_color.w <= 1) base_color.xyz = 1;
		if(base_color.w == 0) base_color = TEX_PT_BASE_COLOR_A[ipos];
		// Project the spherical harmonics lighting onto the actual surface normal
		float3 projected_lf = project_SH_irradiance(filtered_lf, normal);
		projected_lf *= 1024.0f;
		
		float3 final_color = composite_color(base_color, projected_lf, filtered_hf, filtered_spec, (DiffRes && base_color.w <= 1) ? 1 : unpackRGBE(AlbedoColorB[ipos].y), ipos);

		IMG_ASVGF_COLOR[ipos] = float4(final_color, 0);
	}
}

RWTexture2D<float4> Output;


#pragma kernel Finalize
[numthreads(16, 16, 1)]
void Finalize(uint3 gl_GlobalInvocationID : SV_DispatchThreadID, uint3 gl_WorkGroupID : SV_GroupID, uint gl_LocalInvocationIndex : SV_GroupIndex, uint3 gl_LocalInvocationID : SV_GroupThreadID)
{
	RayB[gl_GlobalInvocationID.x + gl_GlobalInvocationID.y * screen_width] = GlobalRays[gl_GlobalInvocationID.x + gl_GlobalInvocationID.y * screen_width];
}


